{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Util"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !python -m pip install tensorflow keras\n",
        "# !python -m pip install scikit-learn\n",
        "# !python -m pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "r13TuBJFtvXQ"
      },
      "outputs": [],
      "source": [
        "# !pip install tensorflow==2.8.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "BQAuZrHDwSLL"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade tensorflow scikeras keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import tensorflow as tf\n",
        "# print(\"Version de TensorFlow:\", tf.__version__)\n",
        "\n",
        "# tf.config.list_physical_devices()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import tensorflow as tf\n",
        "# tf.config.experimental.list_physical_devices()\n",
        "# tf.config.list_physical_devices()\n",
        "# tf.test.gpu_device_name()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import tensorflow as tf\n",
        "# print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qgV6HWc0CBC"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Ql2XMCihytNu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\jiang\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\jiang\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import fasttext.util\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D,Reshape, Dense, Dropout, Flatten,BatchNormalization,MaxPooling1D,SpatialDropout1D, Input, concatenate\n",
        "from keras import regularizers\n",
        "from keras import utils\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "import re\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aE7LmOth00nu"
      },
      "source": [
        "# Coder le texte"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HyXdOTf0H4c"
      },
      "source": [
        "## Récupération des textes et des années de publications (labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def GetDataBooks(chemin):\n",
        "    # Récupérer la liste des noms de fichiers dans le dossier\n",
        "    fichiers = os.listdir(chemin)\n",
        "    \n",
        "    # Initialiser une liste pour stocker les textes\n",
        "    books = []\n",
        "\n",
        "    # Initialiser une liste pour stocker les années extraites des noms de fichiers\n",
        "    y = []\n",
        "\n",
        "    # Parcourir les fichiers et lire leur contenu\n",
        "    for fichier in fichiers:\n",
        "        chemin_fichier = os.path.join(chemin, fichier)\n",
        "        with open(chemin_fichier, 'r', encoding='utf-8') as f:\n",
        "\n",
        "            texte = f.read()\n",
        "\n",
        "            # Extraire l'année du nom de fichier\n",
        "            annee = re.search(r'\\((\\d{4})\\)', fichier)  # Utilisation d'une expression régulière pour trouver l'année entre parenthèses\n",
        "            # Si une année est trouvée, alors nous pouvons étudier le livre, nous l'ajoutons ainsi que son label dans les listes associées\n",
        "            if annee:\n",
        "                books.append(texte)\n",
        "                annee_int = int(annee.group(1)) # Récupérer l'année\n",
        "                y.append(annee_int)  # Ajouter l'année extraite à la liste des labels\n",
        "    return books,y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shbqFxbaboYN",
        "outputId": "e204a37b-37e5-401c-e6be-fd2aa84d648b"
      },
      "outputs": [],
      "source": [
        "# Définition de la seed pour la reproductibilité des résultats\n",
        "utils.set_random_seed(42)\n",
        "\n",
        "# Chemin des dossier contenant les textes\n",
        "dossier_train = 'train'\n",
        "dossier_test = 'test'\n",
        "dossier_val = 'validation'\n",
        "\n",
        "X_train,y_train = GetDataBooks(dossier_train)\n",
        "X_val,y_val = GetDataBooks(dossier_val)\n",
        "X_test,y_test = GetDataBooks(dossier_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1500x500 with 3 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Calculer la plage maximale des valeurs Y sur les trois ensembles de données\n",
        "max_y = max(max(y_train), max(y_val), max(y_test))\n",
        "min_y = min(min(y_train), min(y_val), min(y_test))\n",
        "\n",
        "# Créer des sous-graphiques avec 1 ligne et 3 colonnes\n",
        "fig, axs = plt.subplots(1, 3, figsize=(15, 5), sharey=True)\n",
        "\n",
        "# Tracer l'histogramme pour les données d'entraînement\n",
        "axs[0].hist(y_train, bins=range(min_y, max_y + 1), alpha=0.5, color='skyblue')\n",
        "axs[0].grid(axis='y', linestyle='--')\n",
        "axs[0].set_xlabel('Année')\n",
        "axs[0].set_ylabel('Nombre de textes')\n",
        "axs[0].set_title('Répartition des textes par année (Entraînement)')\n",
        "\n",
        "# Tracer l'histogramme pour les données de validation\n",
        "axs[1].hist(y_val, bins=range(min_y, max_y + 1), alpha=0.5, color='salmon')\n",
        "axs[1].grid(axis='y', linestyle='--')\n",
        "axs[1].set_xlabel('Année')\n",
        "axs[1].set_ylabel('Nombre de textes')\n",
        "axs[1].set_title('Répartition des textes par année (Validation)')\n",
        "\n",
        "# Tracer l'histogramme pour les données de test\n",
        "axs[2].hist(y_test, bins=range(min_y, max_y + 1), alpha=0.5, color='green')\n",
        "axs[2].grid(axis='y', linestyle='--')\n",
        "axs[2].set_xlabel('Année')\n",
        "axs[2].set_ylabel('Nombre de textes')\n",
        "axs[2].set_title('Répartition des textes par année (Test)')\n",
        "\n",
        "# Ajuster la disposition pour éviter les chevauchements\n",
        "plt.tight_layout()\n",
        "\n",
        "# Afficher les graphiques\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qr1HZfr0gkd"
      },
      "source": [
        "## Tokenization + Embedding + division en sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Longueur maximale des séquences\n",
        "max_len = 30000\n",
        "# Le nombre de caracteristique en plus de la taille d'embedding\n",
        "# 3 éléments: le token unique + la frequence locale + la fréquence globale\n",
        "word_param_number = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "embedding_dim = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# #Pout télécharger le model\n",
        "# fasttext.util.download_model('fr', if_exists='ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Récupérer le model\n",
        "# ft = fasttext.load_model('cc.fr.300.bin')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Changer la taille du model (on passe à un embedding de dimension 25)\n",
        "# fasttext.util.reduce_model(ft, embedding_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Récupération de la matrice contenant les embedding de chaque mot\n",
        "# input_matrix = ft.get_input_matrix()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Classe Fonction et Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "_7PvkrqXQRxl"
      },
      "outputs": [],
      "source": [
        "class WordTokenizer:\n",
        "    def __init__(self, dict_word=None, dict_word_frequence=None):\n",
        "        if dict_word is None:\n",
        "            self.dict_word = {}\n",
        "            self.word_freq = {}\n",
        "            self.index = 1\n",
        "        else:\n",
        "            self.dict_word = dict_word\n",
        "            self.index = len(dict_word) + 1\n",
        "            self.word_freq = dict_word_frequence\n",
        "\n",
        "    def fit(self, list_of_texts):\n",
        "        # If dict_word_frequence is None, initialize word_freq\n",
        "        if self.word_freq is None:\n",
        "            self.word_freq = {}\n",
        "\n",
        "        # Calculate word frequency\n",
        "        for text in list_of_texts:\n",
        "            tokens = word_tokenize(text)\n",
        "            for token in tokens:\n",
        "                if token in self.word_freq:\n",
        "                    self.word_freq[token] += 1\n",
        "                else:\n",
        "                    self.word_freq[token] = 1\n",
        "\n",
        "        # Sort words by frequency in descending order\n",
        "        sorted_words = sorted(self.word_freq.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Assign indices based on frequency\n",
        "        for _, (word, _) in enumerate(sorted_words):\n",
        "            if word not in self.dict_word:\n",
        "                self.dict_word[word] = self.index\n",
        "                self.index += 1\n",
        "\n",
        "        print(\"\\nTokenization complete.\")\n",
        "\n",
        "    def get_local_word_freq(self, tokens):\n",
        "        # Initialise un dictionnaire pour stocker la fréquence locale des mots\n",
        "        local_word_freq = {}\n",
        "        # Parcourt tous les tokens dans la liste\n",
        "        for token in tokens:\n",
        "            # Vérifie si le token n'est pas déjà présent dans le dictionnaire\n",
        "            if token not in local_word_freq:\n",
        "                # Si le token n'est pas présent, initialise sa fréquence à 1\n",
        "                local_word_freq[token] = 1\n",
        "            else:\n",
        "                # Si le token est déjà présent, incrémente sa fréquence de 1\n",
        "                local_word_freq[token] += 1\n",
        "        # Retourne le dictionnaire de fréquence locale des mots\n",
        "        return local_word_freq\n",
        "\n",
        "    def tokenize(self, list_of_texts):\n",
        "        total_texts = len(list_of_texts)\n",
        "        # Initialise un tableau pour stocker les tokens de chaque texte\n",
        "        token_arrays = np.empty((total_texts,), dtype=object)\n",
        "        local_occs = np.empty((total_texts,), dtype=object)\n",
        "        token_pos = []\n",
        "        # Parcourt chaque texte dans la liste\n",
        "        for i, text in enumerate(list_of_texts):\n",
        "            # Tokenise le texte en mots individuels\n",
        "            tokens = word_tokenize(text)\n",
        "\n",
        "            # Initialise un tableau pour stocker les informations sur chaque token\n",
        "            token_array = np.empty((len(tokens), ), dtype=np.float32)\n",
        "            local_occ = np.empty((len(tokens), ), dtype=np.float32)\n",
        "            token_pos_arr = np.empty((len(tokens), ), dtype=np.float32)\n",
        "            # Calcule la fréquence locale des mots dans le texte\n",
        "            local_word_freq = self.get_local_word_freq(tokens)\n",
        "\n",
        "            position = 1\n",
        "            # Parcourt chaque token dans le texte\n",
        "            for j, token in enumerate(tokens):\n",
        "                # Position du token\n",
        "                if token == '.':\n",
        "                    position = 0\n",
        "                token_pos_arr[j] = position\n",
        "                #print(\"token :\",token,\" pos :\",position)\n",
        "                position +=1\n",
        "\n",
        "                # Vérifie si le token n'est pas dans le dictionnaire des mots\n",
        "                if token not in self.dict_word:\n",
        "                    token_array[j] = 0\n",
        "                    local_occ[j] = local_word_freq[token]\n",
        "                else:\n",
        "                    token_array[j] = self.dict_word[token]\n",
        "                    local_occ[j] = local_word_freq[token]\n",
        "            \n",
        "            token_arrays[i] = token_array\n",
        "            local_occs[i] = local_occ\n",
        "            token_pos.append(token_pos_arr)\n",
        "            \n",
        "            percentage = (i + 1) / total_texts * 100\n",
        "            print(f\"Progression : {percentage:.2f}% complète\")\n",
        "\n",
        "        print(\"\\nTokenisation terminée.\")\n",
        "        return token_arrays,local_occs, token_pos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "zj_poLDZfP0F"
      },
      "outputs": [],
      "source": [
        "def diviser_liste(liste,labels, taille):\n",
        "    resultats = []\n",
        "    res_labels = []\n",
        "    ind = 0\n",
        "    # Parcours de chaque sous-liste dans la liste principale\n",
        "    for sous_liste in liste:\n",
        "        longueur = len(sous_liste)\n",
        "        if(longueur<taille):\n",
        "            ind+=1\n",
        "            continue\n",
        "        nbtour = round(longueur/taille)\n",
        "        lastpos=0\n",
        "        # Division de la sous-liste en morceaux de taille spécifiée\n",
        "        for i in range(nbtour):\n",
        "            if(lastpos + taille > longueur):\n",
        "              if((lastpos + taille)-longueur < max_len/2):\n",
        "                  continue\n",
        "              lastpos -= (lastpos + taille)-longueur\n",
        "\n",
        "            # Ajout du morceau à la liste de résultats\n",
        "            resultat = sous_liste[lastpos:lastpos + taille]\n",
        "            lastpos = lastpos + taille\n",
        "            resultats.append(resultat)\n",
        "\n",
        "            # A chaque fois que je divisie, jajoute le label au meme indice\n",
        "            res_labels.append(labels[ind])\n",
        "        ind+=1\n",
        "\n",
        "    # Convertir resultats et res_labels en tableaux numpy\n",
        "    resultats = np.array(resultats)\n",
        "    res_labels = np.array(res_labels)\n",
        "\n",
        "    return resultats,res_labels\n",
        "\n",
        "def diviser_liste2(liste,taille):\n",
        "    resultats = []\n",
        "    ind = 0\n",
        "    # Parcours de chaque sous-liste dans la liste principale\n",
        "    for sous_liste in liste:\n",
        "        longueur = len(sous_liste)\n",
        "        if(longueur<taille):\n",
        "            ind+=1\n",
        "            continue\n",
        "        nbtour = round(longueur/taille)\n",
        "        lastpos=0\n",
        "        # Division de la sous-liste en morceaux de taille spécifiée\n",
        "        for i in range(nbtour):\n",
        "            if(lastpos + taille > longueur):\n",
        "              if((lastpos + taille)-longueur < max_len/2):\n",
        "                  continue\n",
        "              lastpos -= (lastpos + taille)-longueur\n",
        "\n",
        "            # Ajout du morceau à la liste de résultats\n",
        "            resultat = sous_liste[lastpos:lastpos + taille]\n",
        "            lastpos = lastpos + taille\n",
        "            resultats.append(resultat)\n",
        "        ind+=1\n",
        "\n",
        "    # Convertir resultats et res_labels en tableaux numpy\n",
        "    resultats = np.array(resultats)\n",
        "\n",
        "    return resultats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Création du tokenizer + sauvegarde du dictionnaire"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tokenizer = WordTokenizer()\n",
        "# tokenizer.fit(X_train)\n",
        "# tokenizer.fit(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import json\n",
        "\n",
        "# # Chemin du fichier de sauvegarde\n",
        "# path_dict_word = \"dict_word.json\"\n",
        "# path_dict_word_freq = \"dict_word_freq.json\"\n",
        "\n",
        "# # Sauvegarde du dictionnaire en JSON dans un fichier\n",
        "# with open(path_dict_word, \"w\") as json_file:\n",
        "#     json.dump(tokenizer.dict_word, json_file)\n",
        "# with open(path_dict_word_freq, \"w\") as json_file:\n",
        "#     json.dump(tokenizer.word_freq, json_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Récupération dico + Création du tokenizer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Chemin du fichier JSON à lire\n",
        "path_dict_word = \"dict_word.json\"\n",
        "path_dict_word_freq = \"dict_word_freq.json\"\n",
        "\n",
        "ditc_word = {}\n",
        "word_freq = {}\n",
        "\n",
        "# Lecture du fichier JSON\n",
        "with open(path_dict_word, \"r\") as json_file:\n",
        "    ditc_word = json.load(json_file)\n",
        "with open(path_dict_word_freq, \"r\") as json_file:\n",
        "    word_freq = json.load(json_file)\n",
        "tokenizer = WordTokenizer(dict_word=ditc_word,dict_word_frequence=word_freq)\n",
        "\n",
        "token_number = tokenizer.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{',': 1, '.': 2, 'de': 3, 'la': 4, 'et': 5, 'le': 6, 'à': 7, 'il': 8, 'l': 9, 'les': 10, 'un': 11, 'que': 12, 'd': 13, 'en': 14, 'je': 15, 'une': 16, 'elle': 17, 'des': 18, 'qui': 19, 'pas': 20, 'est': 21, 'qu': 22, 'ne': 23, 'dans': 24, '!': 25, 'se': 26, 'ce': 27, 'vous': 28, 'pour': 29, 'du': 30, 'n': 31, 's': 32, '?': 33, 'son': 34, 'était': 35, 'au': 36, 'lui': 37, 'plus': 38, 'avait': 39, 'mais': 40, ';': 41, 'a': 42, 'sur': 43, 'on': 44, ':': 45, 'c': 46, 'sa': 47, 'avec': 48, 'par': 49, 'nous': 50, 'j': 51, 'tout': 52, 'me': 53, 'comme': 54, 'ses': 55, 'si': 56, 'bien': 57, 'm': 58, 'y': 59, 'cette': 60, 'tu': 61, 'dit': 62, 'ils': 63, 'mon': 64, 'ai': 65, 'même': 66, 'sans': 67, 'moi': 68, 'être': 69, 'où': 70, 'ou': 71, 'fait': 72, 'deux': 73, 'aux': 74, 'là': 75, 'ces': 76, 'ma': 77, 'faire': 78, 'leur': 79, 't': 80, 'encore': 81, 'quand': 82, 'rien': 83, 'peu': 84, 'aussi': 85, 'tous': 86, 'autre': 87, 'homme': 88, 'ça': 89, 'peut': 90, 'sont': 91, 'temps': 92, 'non': 93, 'été': 94, 'jamais': 95, 'puis': 96, 'après': 97, 'suis': 98, 'dont': 99, 'avoir': 100}\n"
          ]
        }
      ],
      "source": [
        "# Afficher les 100 premiers éléments\n",
        "first_100 = {word: tokenizer.dict_word[word] for word in list(tokenizer.dict_word.keys())[:100]}\n",
        "print(first_100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "NwFu_20zSujU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progression : 0.12% complète\n",
            "Progression : 0.24% complète\n",
            "Progression : 0.37% complète\n",
            "Progression : 0.49% complète\n",
            "Progression : 0.61% complète\n",
            "Progression : 0.73% complète\n",
            "Progression : 0.85% complète\n",
            "Progression : 0.98% complète\n",
            "Progression : 1.10% complète\n",
            "Progression : 1.22% complète\n",
            "Progression : 1.34% complète\n",
            "Progression : 1.46% complète\n",
            "Progression : 1.59% complète\n",
            "Progression : 1.71% complète\n",
            "Progression : 1.83% complète\n",
            "Progression : 1.95% complète\n",
            "Progression : 2.07% complète\n",
            "Progression : 2.20% complète\n",
            "Progression : 2.32% complète\n",
            "Progression : 2.44% complète\n",
            "Progression : 2.56% complète\n",
            "Progression : 2.68% complète\n",
            "Progression : 2.80% complète\n",
            "Progression : 2.93% complète\n",
            "Progression : 3.05% complète\n",
            "Progression : 3.17% complète\n",
            "Progression : 3.29% complète\n",
            "Progression : 3.41% complète\n",
            "Progression : 3.54% complète\n",
            "Progression : 3.66% complète\n",
            "Progression : 3.78% complète\n",
            "Progression : 3.90% complète\n",
            "Progression : 4.02% complète\n",
            "Progression : 4.15% complète\n",
            "Progression : 4.27% complète\n",
            "Progression : 4.39% complète\n",
            "Progression : 4.51% complète\n",
            "Progression : 4.63% complète\n",
            "Progression : 4.76% complète\n",
            "Progression : 4.88% complète\n",
            "Progression : 5.00% complète\n",
            "Progression : 5.12% complète\n",
            "Progression : 5.24% complète\n",
            "Progression : 5.37% complète\n",
            "Progression : 5.49% complète\n",
            "Progression : 5.61% complète\n",
            "Progression : 5.73% complète\n",
            "Progression : 5.85% complète\n",
            "Progression : 5.98% complète\n",
            "Progression : 6.10% complète\n",
            "Progression : 6.22% complète\n",
            "Progression : 6.34% complète\n",
            "Progression : 6.46% complète\n",
            "Progression : 6.59% complète\n",
            "Progression : 6.71% complète\n",
            "Progression : 6.83% complète\n",
            "Progression : 6.95% complète\n",
            "Progression : 7.07% complète\n",
            "Progression : 7.20% complète\n",
            "Progression : 7.32% complète\n",
            "Progression : 7.44% complète\n",
            "Progression : 7.56% complète\n",
            "Progression : 7.68% complète\n",
            "Progression : 7.80% complète\n",
            "Progression : 7.93% complète\n",
            "Progression : 8.05% complète\n",
            "Progression : 8.17% complète\n",
            "Progression : 8.29% complète\n",
            "Progression : 8.41% complète\n",
            "Progression : 8.54% complète\n",
            "Progression : 8.66% complète\n",
            "Progression : 8.78% complète\n",
            "Progression : 8.90% complète\n",
            "Progression : 9.02% complète\n",
            "Progression : 9.15% complète\n",
            "Progression : 9.27% complète\n",
            "Progression : 9.39% complète\n",
            "Progression : 9.51% complète\n",
            "Progression : 9.63% complète\n",
            "Progression : 9.76% complète\n",
            "Progression : 9.88% complète\n",
            "Progression : 10.00% complète\n",
            "Progression : 10.12% complète\n",
            "Progression : 10.24% complète\n",
            "Progression : 10.37% complète\n",
            "Progression : 10.49% complète\n",
            "Progression : 10.61% complète\n",
            "Progression : 10.73% complète\n",
            "Progression : 10.85% complète\n",
            "Progression : 10.98% complète\n",
            "Progression : 11.10% complète\n",
            "Progression : 11.22% complète\n",
            "Progression : 11.34% complète\n",
            "Progression : 11.46% complète\n",
            "Progression : 11.59% complète\n",
            "Progression : 11.71% complète\n",
            "Progression : 11.83% complète\n",
            "Progression : 11.95% complète\n",
            "Progression : 12.07% complète\n",
            "Progression : 12.20% complète\n",
            "Progression : 12.32% complète\n",
            "Progression : 12.44% complète\n",
            "Progression : 12.56% complète\n",
            "Progression : 12.68% complète\n",
            "Progression : 12.80% complète\n",
            "Progression : 12.93% complète\n",
            "Progression : 13.05% complète\n",
            "Progression : 13.17% complète\n",
            "Progression : 13.29% complète\n",
            "Progression : 13.41% complète\n",
            "Progression : 13.54% complète\n",
            "Progression : 13.66% complète\n",
            "Progression : 13.78% complète\n",
            "Progression : 13.90% complète\n",
            "Progression : 14.02% complète\n",
            "Progression : 14.15% complète\n",
            "Progression : 14.27% complète\n",
            "Progression : 14.39% complète\n",
            "Progression : 14.51% complète\n",
            "Progression : 14.63% complète\n",
            "Progression : 14.76% complète\n",
            "Progression : 14.88% complète\n",
            "Progression : 15.00% complète\n",
            "Progression : 15.12% complète\n",
            "Progression : 15.24% complète\n",
            "Progression : 15.37% complète\n",
            "Progression : 15.49% complète\n",
            "Progression : 15.61% complète\n",
            "Progression : 15.73% complète\n",
            "Progression : 15.85% complète\n",
            "Progression : 15.98% complète\n",
            "Progression : 16.10% complète\n",
            "Progression : 16.22% complète\n",
            "Progression : 16.34% complète\n",
            "Progression : 16.46% complète\n",
            "Progression : 16.59% complète\n",
            "Progression : 16.71% complète\n",
            "Progression : 16.83% complète\n",
            "Progression : 16.95% complète\n",
            "Progression : 17.07% complète\n",
            "Progression : 17.20% complète\n",
            "Progression : 17.32% complète\n",
            "Progression : 17.44% complète\n",
            "Progression : 17.56% complète\n",
            "Progression : 17.68% complète\n",
            "Progression : 17.80% complète\n",
            "Progression : 17.93% complète\n",
            "Progression : 18.05% complète\n",
            "Progression : 18.17% complète\n",
            "Progression : 18.29% complète\n",
            "Progression : 18.41% complète\n",
            "Progression : 18.54% complète\n",
            "Progression : 18.66% complète\n",
            "Progression : 18.78% complète\n",
            "Progression : 18.90% complète\n",
            "Progression : 19.02% complète\n",
            "Progression : 19.15% complète\n",
            "Progression : 19.27% complète\n",
            "Progression : 19.39% complète\n",
            "Progression : 19.51% complète\n",
            "Progression : 19.63% complète\n",
            "Progression : 19.76% complète\n",
            "Progression : 19.88% complète\n",
            "Progression : 20.00% complète\n",
            "Progression : 20.12% complète\n",
            "Progression : 20.24% complète\n",
            "Progression : 20.37% complète\n",
            "Progression : 20.49% complète\n",
            "Progression : 20.61% complète\n",
            "Progression : 20.73% complète\n",
            "Progression : 20.85% complète\n",
            "Progression : 20.98% complète\n",
            "Progression : 21.10% complète\n",
            "Progression : 21.22% complète\n",
            "Progression : 21.34% complète\n",
            "Progression : 21.46% complète\n",
            "Progression : 21.59% complète\n",
            "Progression : 21.71% complète\n",
            "Progression : 21.83% complète\n",
            "Progression : 21.95% complète\n",
            "Progression : 22.07% complète\n",
            "Progression : 22.20% complète\n",
            "Progression : 22.32% complète\n",
            "Progression : 22.44% complète\n",
            "Progression : 22.56% complète\n",
            "Progression : 22.68% complète\n",
            "Progression : 22.80% complète\n",
            "Progression : 22.93% complète\n",
            "Progression : 23.05% complète\n",
            "Progression : 23.17% complète\n",
            "Progression : 23.29% complète\n",
            "Progression : 23.41% complète\n",
            "Progression : 23.54% complète\n",
            "Progression : 23.66% complète\n",
            "Progression : 23.78% complète\n",
            "Progression : 23.90% complète\n",
            "Progression : 24.02% complète\n",
            "Progression : 24.15% complète\n",
            "Progression : 24.27% complète\n",
            "Progression : 24.39% complète\n",
            "Progression : 24.51% complète\n",
            "Progression : 24.63% complète\n",
            "Progression : 24.76% complète\n",
            "Progression : 24.88% complète\n",
            "Progression : 25.00% complète\n",
            "Progression : 25.12% complète\n",
            "Progression : 25.24% complète\n",
            "Progression : 25.37% complète\n",
            "Progression : 25.49% complète\n",
            "Progression : 25.61% complète\n",
            "Progression : 25.73% complète\n",
            "Progression : 25.85% complète\n",
            "Progression : 25.98% complète\n",
            "Progression : 26.10% complète\n",
            "Progression : 26.22% complète\n",
            "Progression : 26.34% complète\n",
            "Progression : 26.46% complète\n",
            "Progression : 26.59% complète\n",
            "Progression : 26.71% complète\n",
            "Progression : 26.83% complète\n",
            "Progression : 26.95% complète\n",
            "Progression : 27.07% complète\n",
            "Progression : 27.20% complète\n",
            "Progression : 27.32% complète\n",
            "Progression : 27.44% complète\n",
            "Progression : 27.56% complète\n",
            "Progression : 27.68% complète\n",
            "Progression : 27.80% complète\n",
            "Progression : 27.93% complète\n",
            "Progression : 28.05% complète\n",
            "Progression : 28.17% complète\n",
            "Progression : 28.29% complète\n",
            "Progression : 28.41% complète\n",
            "Progression : 28.54% complète\n",
            "Progression : 28.66% complète\n",
            "Progression : 28.78% complète\n",
            "Progression : 28.90% complète\n",
            "Progression : 29.02% complète\n",
            "Progression : 29.15% complète\n",
            "Progression : 29.27% complète\n",
            "Progression : 29.39% complète\n",
            "Progression : 29.51% complète\n",
            "Progression : 29.63% complète\n",
            "Progression : 29.76% complète\n",
            "Progression : 29.88% complète\n",
            "Progression : 30.00% complète\n",
            "Progression : 30.12% complète\n",
            "Progression : 30.24% complète\n",
            "Progression : 30.37% complète\n",
            "Progression : 30.49% complète\n",
            "Progression : 30.61% complète\n",
            "Progression : 30.73% complète\n",
            "Progression : 30.85% complète\n",
            "Progression : 30.98% complète\n",
            "Progression : 31.10% complète\n",
            "Progression : 31.22% complète\n",
            "Progression : 31.34% complète\n",
            "Progression : 31.46% complète\n",
            "Progression : 31.59% complète\n",
            "Progression : 31.71% complète\n",
            "Progression : 31.83% complète\n",
            "Progression : 31.95% complète\n",
            "Progression : 32.07% complète\n",
            "Progression : 32.20% complète\n",
            "Progression : 32.32% complète\n",
            "Progression : 32.44% complète\n",
            "Progression : 32.56% complète\n",
            "Progression : 32.68% complète\n",
            "Progression : 32.80% complète\n",
            "Progression : 32.93% complète\n",
            "Progression : 33.05% complète\n",
            "Progression : 33.17% complète\n",
            "Progression : 33.29% complète\n",
            "Progression : 33.41% complète\n",
            "Progression : 33.54% complète\n",
            "Progression : 33.66% complète\n",
            "Progression : 33.78% complète\n",
            "Progression : 33.90% complète\n",
            "Progression : 34.02% complète\n",
            "Progression : 34.15% complète\n",
            "Progression : 34.27% complète\n",
            "Progression : 34.39% complète\n",
            "Progression : 34.51% complète\n",
            "Progression : 34.63% complète\n",
            "Progression : 34.76% complète\n",
            "Progression : 34.88% complète\n",
            "Progression : 35.00% complète\n",
            "Progression : 35.12% complète\n",
            "Progression : 35.24% complète\n",
            "Progression : 35.37% complète\n",
            "Progression : 35.49% complète\n",
            "Progression : 35.61% complète\n",
            "Progression : 35.73% complète\n",
            "Progression : 35.85% complète\n",
            "Progression : 35.98% complète\n",
            "Progression : 36.10% complète\n",
            "Progression : 36.22% complète\n",
            "Progression : 36.34% complète\n",
            "Progression : 36.46% complète\n",
            "Progression : 36.59% complète\n",
            "Progression : 36.71% complète\n",
            "Progression : 36.83% complète\n",
            "Progression : 36.95% complète\n",
            "Progression : 37.07% complète\n",
            "Progression : 37.20% complète\n",
            "Progression : 37.32% complète\n",
            "Progression : 37.44% complète\n",
            "Progression : 37.56% complète\n",
            "Progression : 37.68% complète\n",
            "Progression : 37.80% complète\n",
            "Progression : 37.93% complète\n",
            "Progression : 38.05% complète\n",
            "Progression : 38.17% complète\n",
            "Progression : 38.29% complète\n",
            "Progression : 38.41% complète\n",
            "Progression : 38.54% complète\n",
            "Progression : 38.66% complète\n",
            "Progression : 38.78% complète\n",
            "Progression : 38.90% complète\n",
            "Progression : 39.02% complète\n",
            "Progression : 39.15% complète\n",
            "Progression : 39.27% complète\n",
            "Progression : 39.39% complète\n",
            "Progression : 39.51% complète\n",
            "Progression : 39.63% complète\n",
            "Progression : 39.76% complète\n",
            "Progression : 39.88% complète\n",
            "Progression : 40.00% complète\n",
            "Progression : 40.12% complète\n",
            "Progression : 40.24% complète\n",
            "Progression : 40.37% complète\n",
            "Progression : 40.49% complète\n",
            "Progression : 40.61% complète\n",
            "Progression : 40.73% complète\n",
            "Progression : 40.85% complète\n",
            "Progression : 40.98% complète\n",
            "Progression : 41.10% complète\n",
            "Progression : 41.22% complète\n",
            "Progression : 41.34% complète\n",
            "Progression : 41.46% complète\n",
            "Progression : 41.59% complète\n",
            "Progression : 41.71% complète\n",
            "Progression : 41.83% complète\n",
            "Progression : 41.95% complète\n",
            "Progression : 42.07% complète\n",
            "Progression : 42.20% complète\n",
            "Progression : 42.32% complète\n",
            "Progression : 42.44% complète\n",
            "Progression : 42.56% complète\n",
            "Progression : 42.68% complète\n",
            "Progression : 42.80% complète\n",
            "Progression : 42.93% complète\n",
            "Progression : 43.05% complète\n",
            "Progression : 43.17% complète\n",
            "Progression : 43.29% complète\n",
            "Progression : 43.41% complète\n",
            "Progression : 43.54% complète\n",
            "Progression : 43.66% complète\n",
            "Progression : 43.78% complète\n",
            "Progression : 43.90% complète\n",
            "Progression : 44.02% complète\n",
            "Progression : 44.15% complète\n",
            "Progression : 44.27% complète\n",
            "Progression : 44.39% complète\n",
            "Progression : 44.51% complète\n",
            "Progression : 44.63% complète\n",
            "Progression : 44.76% complète\n",
            "Progression : 44.88% complète\n",
            "Progression : 45.00% complète\n",
            "Progression : 45.12% complète\n",
            "Progression : 45.24% complète\n",
            "Progression : 45.37% complète\n",
            "Progression : 45.49% complète\n",
            "Progression : 45.61% complète\n",
            "Progression : 45.73% complète\n",
            "Progression : 45.85% complète\n",
            "Progression : 45.98% complète\n",
            "Progression : 46.10% complète\n",
            "Progression : 46.22% complète\n",
            "Progression : 46.34% complète\n",
            "Progression : 46.46% complète\n",
            "Progression : 46.59% complète\n",
            "Progression : 46.71% complète\n",
            "Progression : 46.83% complète\n",
            "Progression : 46.95% complète\n",
            "Progression : 47.07% complète\n",
            "Progression : 47.20% complète\n",
            "Progression : 47.32% complète\n",
            "Progression : 47.44% complète\n",
            "Progression : 47.56% complète\n",
            "Progression : 47.68% complète\n",
            "Progression : 47.80% complète\n",
            "Progression : 47.93% complète\n",
            "Progression : 48.05% complète\n",
            "Progression : 48.17% complète\n",
            "Progression : 48.29% complète\n",
            "Progression : 48.41% complète\n",
            "Progression : 48.54% complète\n",
            "Progression : 48.66% complète\n",
            "Progression : 48.78% complète\n",
            "Progression : 48.90% complète\n",
            "Progression : 49.02% complète\n",
            "Progression : 49.15% complète\n",
            "Progression : 49.27% complète\n",
            "Progression : 49.39% complète\n",
            "Progression : 49.51% complète\n",
            "Progression : 49.63% complète\n",
            "Progression : 49.76% complète\n",
            "Progression : 49.88% complète\n",
            "Progression : 50.00% complète\n",
            "Progression : 50.12% complète\n",
            "Progression : 50.24% complète\n",
            "Progression : 50.37% complète\n",
            "Progression : 50.49% complète\n",
            "Progression : 50.61% complète\n",
            "Progression : 50.73% complète\n",
            "Progression : 50.85% complète\n",
            "Progression : 50.98% complète\n",
            "Progression : 51.10% complète\n",
            "Progression : 51.22% complète\n",
            "Progression : 51.34% complète\n",
            "Progression : 51.46% complète\n",
            "Progression : 51.59% complète\n",
            "Progression : 51.71% complète\n",
            "Progression : 51.83% complète\n",
            "Progression : 51.95% complète\n",
            "Progression : 52.07% complète\n",
            "Progression : 52.20% complète\n",
            "Progression : 52.32% complète\n",
            "Progression : 52.44% complète\n",
            "Progression : 52.56% complète\n",
            "Progression : 52.68% complète\n",
            "Progression : 52.80% complète\n",
            "Progression : 52.93% complète\n",
            "Progression : 53.05% complète\n",
            "Progression : 53.17% complète\n",
            "Progression : 53.29% complète\n",
            "Progression : 53.41% complète\n",
            "Progression : 53.54% complète\n",
            "Progression : 53.66% complète\n",
            "Progression : 53.78% complète\n",
            "Progression : 53.90% complète\n",
            "Progression : 54.02% complète\n",
            "Progression : 54.15% complète\n",
            "Progression : 54.27% complète\n",
            "Progression : 54.39% complète\n",
            "Progression : 54.51% complète\n",
            "Progression : 54.63% complète\n",
            "Progression : 54.76% complète\n",
            "Progression : 54.88% complète\n",
            "Progression : 55.00% complète\n",
            "Progression : 55.12% complète\n",
            "Progression : 55.24% complète\n",
            "Progression : 55.37% complète\n",
            "Progression : 55.49% complète\n",
            "Progression : 55.61% complète\n",
            "Progression : 55.73% complète\n",
            "Progression : 55.85% complète\n",
            "Progression : 55.98% complète\n",
            "Progression : 56.10% complète\n",
            "Progression : 56.22% complète\n",
            "Progression : 56.34% complète\n",
            "Progression : 56.46% complète\n",
            "Progression : 56.59% complète\n",
            "Progression : 56.71% complète\n",
            "Progression : 56.83% complète\n",
            "Progression : 56.95% complète\n",
            "Progression : 57.07% complète\n",
            "Progression : 57.20% complète\n",
            "Progression : 57.32% complète\n",
            "Progression : 57.44% complète\n",
            "Progression : 57.56% complète\n",
            "Progression : 57.68% complète\n",
            "Progression : 57.80% complète\n",
            "Progression : 57.93% complète\n",
            "Progression : 58.05% complète\n",
            "Progression : 58.17% complète\n",
            "Progression : 58.29% complète\n",
            "Progression : 58.41% complète\n",
            "Progression : 58.54% complète\n",
            "Progression : 58.66% complète\n",
            "Progression : 58.78% complète\n",
            "Progression : 58.90% complète\n",
            "Progression : 59.02% complète\n",
            "Progression : 59.15% complète\n",
            "Progression : 59.27% complète\n",
            "Progression : 59.39% complète\n",
            "Progression : 59.51% complète\n",
            "Progression : 59.63% complète\n",
            "Progression : 59.76% complète\n",
            "Progression : 59.88% complète\n",
            "Progression : 60.00% complète\n",
            "Progression : 60.12% complète\n",
            "Progression : 60.24% complète\n",
            "Progression : 60.37% complète\n",
            "Progression : 60.49% complète\n",
            "Progression : 60.61% complète\n",
            "Progression : 60.73% complète\n",
            "Progression : 60.85% complète\n",
            "Progression : 60.98% complète\n",
            "Progression : 61.10% complète\n",
            "Progression : 61.22% complète\n",
            "Progression : 61.34% complète\n",
            "Progression : 61.46% complète\n",
            "Progression : 61.59% complète\n",
            "Progression : 61.71% complète\n",
            "Progression : 61.83% complète\n",
            "Progression : 61.95% complète\n",
            "Progression : 62.07% complète\n",
            "Progression : 62.20% complète\n",
            "Progression : 62.32% complète\n",
            "Progression : 62.44% complète\n",
            "Progression : 62.56% complète\n",
            "Progression : 62.68% complète\n",
            "Progression : 62.80% complète\n",
            "Progression : 62.93% complète\n",
            "Progression : 63.05% complète\n",
            "Progression : 63.17% complète\n",
            "Progression : 63.29% complète\n",
            "Progression : 63.41% complète\n",
            "Progression : 63.54% complète\n",
            "Progression : 63.66% complète\n",
            "Progression : 63.78% complète\n",
            "Progression : 63.90% complète\n",
            "Progression : 64.02% complète\n",
            "Progression : 64.15% complète\n",
            "Progression : 64.27% complète\n",
            "Progression : 64.39% complète\n",
            "Progression : 64.51% complète\n",
            "Progression : 64.63% complète\n",
            "Progression : 64.76% complète\n",
            "Progression : 64.88% complète\n",
            "Progression : 65.00% complète\n",
            "Progression : 65.12% complète\n",
            "Progression : 65.24% complète\n",
            "Progression : 65.37% complète\n",
            "Progression : 65.49% complète\n",
            "Progression : 65.61% complète\n",
            "Progression : 65.73% complète\n",
            "Progression : 65.85% complète\n",
            "Progression : 65.98% complète\n",
            "Progression : 66.10% complète\n",
            "Progression : 66.22% complète\n",
            "Progression : 66.34% complète\n",
            "Progression : 66.46% complète\n",
            "Progression : 66.59% complète\n",
            "Progression : 66.71% complète\n",
            "Progression : 66.83% complète\n",
            "Progression : 66.95% complète\n",
            "Progression : 67.07% complète\n",
            "Progression : 67.20% complète\n",
            "Progression : 67.32% complète\n",
            "Progression : 67.44% complète\n",
            "Progression : 67.56% complète\n",
            "Progression : 67.68% complète\n",
            "Progression : 67.80% complète\n",
            "Progression : 67.93% complète\n",
            "Progression : 68.05% complète\n",
            "Progression : 68.17% complète\n",
            "Progression : 68.29% complète\n",
            "Progression : 68.41% complète\n",
            "Progression : 68.54% complète\n",
            "Progression : 68.66% complète\n",
            "Progression : 68.78% complète\n",
            "Progression : 68.90% complète\n",
            "Progression : 69.02% complète\n",
            "Progression : 69.15% complète\n",
            "Progression : 69.27% complète\n",
            "Progression : 69.39% complète\n",
            "Progression : 69.51% complète\n",
            "Progression : 69.63% complète\n",
            "Progression : 69.76% complète\n",
            "Progression : 69.88% complète\n",
            "Progression : 70.00% complète\n",
            "Progression : 70.12% complète\n",
            "Progression : 70.24% complète\n",
            "Progression : 70.37% complète\n",
            "Progression : 70.49% complète\n",
            "Progression : 70.61% complète\n",
            "Progression : 70.73% complète\n",
            "Progression : 70.85% complète\n",
            "Progression : 70.98% complète\n",
            "Progression : 71.10% complète\n",
            "Progression : 71.22% complète\n",
            "Progression : 71.34% complète\n",
            "Progression : 71.46% complète\n",
            "Progression : 71.59% complète\n",
            "Progression : 71.71% complète\n",
            "Progression : 71.83% complète\n",
            "Progression : 71.95% complète\n",
            "Progression : 72.07% complète\n",
            "Progression : 72.20% complète\n",
            "Progression : 72.32% complète\n",
            "Progression : 72.44% complète\n",
            "Progression : 72.56% complète\n",
            "Progression : 72.68% complète\n",
            "Progression : 72.80% complète\n",
            "Progression : 72.93% complète\n",
            "Progression : 73.05% complète\n",
            "Progression : 73.17% complète\n",
            "Progression : 73.29% complète\n",
            "Progression : 73.41% complète\n",
            "Progression : 73.54% complète\n",
            "Progression : 73.66% complète\n",
            "Progression : 73.78% complète\n",
            "Progression : 73.90% complète\n",
            "Progression : 74.02% complète\n",
            "Progression : 74.15% complète\n",
            "Progression : 74.27% complète\n",
            "Progression : 74.39% complète\n",
            "Progression : 74.51% complète\n",
            "Progression : 74.63% complète\n",
            "Progression : 74.76% complète\n",
            "Progression : 74.88% complète\n",
            "Progression : 75.00% complète\n",
            "Progression : 75.12% complète\n",
            "Progression : 75.24% complète\n",
            "Progression : 75.37% complète\n",
            "Progression : 75.49% complète\n",
            "Progression : 75.61% complète\n",
            "Progression : 75.73% complète\n",
            "Progression : 75.85% complète\n",
            "Progression : 75.98% complète\n",
            "Progression : 76.10% complète\n",
            "Progression : 76.22% complète\n",
            "Progression : 76.34% complète\n",
            "Progression : 76.46% complète\n",
            "Progression : 76.59% complète\n",
            "Progression : 76.71% complète\n",
            "Progression : 76.83% complète\n",
            "Progression : 76.95% complète\n",
            "Progression : 77.07% complète\n",
            "Progression : 77.20% complète\n",
            "Progression : 77.32% complète\n",
            "Progression : 77.44% complète\n",
            "Progression : 77.56% complète\n",
            "Progression : 77.68% complète\n",
            "Progression : 77.80% complète\n",
            "Progression : 77.93% complète\n",
            "Progression : 78.05% complète\n",
            "Progression : 78.17% complète\n",
            "Progression : 78.29% complète\n",
            "Progression : 78.41% complète\n",
            "Progression : 78.54% complète\n",
            "Progression : 78.66% complète\n",
            "Progression : 78.78% complète\n",
            "Progression : 78.90% complète\n",
            "Progression : 79.02% complète\n",
            "Progression : 79.15% complète\n",
            "Progression : 79.27% complète\n",
            "Progression : 79.39% complète\n",
            "Progression : 79.51% complète\n",
            "Progression : 79.63% complète\n",
            "Progression : 79.76% complète\n",
            "Progression : 79.88% complète\n",
            "Progression : 80.00% complète\n",
            "Progression : 80.12% complète\n",
            "Progression : 80.24% complète\n",
            "Progression : 80.37% complète\n",
            "Progression : 80.49% complète\n",
            "Progression : 80.61% complète\n",
            "Progression : 80.73% complète\n",
            "Progression : 80.85% complète\n",
            "Progression : 80.98% complète\n",
            "Progression : 81.10% complète\n",
            "Progression : 81.22% complète\n",
            "Progression : 81.34% complète\n",
            "Progression : 81.46% complète\n",
            "Progression : 81.59% complète\n",
            "Progression : 81.71% complète\n",
            "Progression : 81.83% complète\n",
            "Progression : 81.95% complète\n",
            "Progression : 82.07% complète\n",
            "Progression : 82.20% complète\n",
            "Progression : 82.32% complète\n",
            "Progression : 82.44% complète\n",
            "Progression : 82.56% complète\n",
            "Progression : 82.68% complète\n",
            "Progression : 82.80% complète\n",
            "Progression : 82.93% complète\n",
            "Progression : 83.05% complète\n",
            "Progression : 83.17% complète\n",
            "Progression : 83.29% complète\n",
            "Progression : 83.41% complète\n",
            "Progression : 83.54% complète\n",
            "Progression : 83.66% complète\n",
            "Progression : 83.78% complète\n",
            "Progression : 83.90% complète\n",
            "Progression : 84.02% complète\n",
            "Progression : 84.15% complète\n",
            "Progression : 84.27% complète\n",
            "Progression : 84.39% complète\n",
            "Progression : 84.51% complète\n",
            "Progression : 84.63% complète\n",
            "Progression : 84.76% complète\n",
            "Progression : 84.88% complète\n",
            "Progression : 85.00% complète\n",
            "Progression : 85.12% complète\n",
            "Progression : 85.24% complète\n",
            "Progression : 85.37% complète\n",
            "Progression : 85.49% complète\n",
            "Progression : 85.61% complète\n",
            "Progression : 85.73% complète\n",
            "Progression : 85.85% complète\n",
            "Progression : 85.98% complète\n",
            "Progression : 86.10% complète\n",
            "Progression : 86.22% complète\n",
            "Progression : 86.34% complète\n",
            "Progression : 86.46% complète\n",
            "Progression : 86.59% complète\n",
            "Progression : 86.71% complète\n",
            "Progression : 86.83% complète\n",
            "Progression : 86.95% complète\n",
            "Progression : 87.07% complète\n",
            "Progression : 87.20% complète\n",
            "Progression : 87.32% complète\n",
            "Progression : 87.44% complète\n",
            "Progression : 87.56% complète\n",
            "Progression : 87.68% complète\n",
            "Progression : 87.80% complète\n",
            "Progression : 87.93% complète\n",
            "Progression : 88.05% complète\n",
            "Progression : 88.17% complète\n",
            "Progression : 88.29% complète\n",
            "Progression : 88.41% complète\n",
            "Progression : 88.54% complète\n",
            "Progression : 88.66% complète\n",
            "Progression : 88.78% complète\n",
            "Progression : 88.90% complète\n",
            "Progression : 89.02% complète\n",
            "Progression : 89.15% complète\n",
            "Progression : 89.27% complète\n",
            "Progression : 89.39% complète\n",
            "Progression : 89.51% complète\n",
            "Progression : 89.63% complète\n",
            "Progression : 89.76% complète\n",
            "Progression : 89.88% complète\n",
            "Progression : 90.00% complète\n",
            "Progression : 90.12% complète\n",
            "Progression : 90.24% complète\n",
            "Progression : 90.37% complète\n",
            "Progression : 90.49% complète\n",
            "Progression : 90.61% complète\n",
            "Progression : 90.73% complète\n",
            "Progression : 90.85% complète\n",
            "Progression : 90.98% complète\n",
            "Progression : 91.10% complète\n",
            "Progression : 91.22% complète\n",
            "Progression : 91.34% complète\n",
            "Progression : 91.46% complète\n",
            "Progression : 91.59% complète\n",
            "Progression : 91.71% complète\n",
            "Progression : 91.83% complète\n",
            "Progression : 91.95% complète\n",
            "Progression : 92.07% complète\n",
            "Progression : 92.20% complète\n",
            "Progression : 92.32% complète\n",
            "Progression : 92.44% complète\n",
            "Progression : 92.56% complète\n",
            "Progression : 92.68% complète\n",
            "Progression : 92.80% complète\n",
            "Progression : 92.93% complète\n",
            "Progression : 93.05% complète\n",
            "Progression : 93.17% complète\n",
            "Progression : 93.29% complète\n",
            "Progression : 93.41% complète\n",
            "Progression : 93.54% complète\n",
            "Progression : 93.66% complète\n",
            "Progression : 93.78% complète\n",
            "Progression : 93.90% complète\n",
            "Progression : 94.02% complète\n",
            "Progression : 94.15% complète\n",
            "Progression : 94.27% complète\n",
            "Progression : 94.39% complète\n",
            "Progression : 94.51% complète\n",
            "Progression : 94.63% complète\n",
            "Progression : 94.76% complète\n",
            "Progression : 94.88% complète\n",
            "Progression : 95.00% complète\n",
            "Progression : 95.12% complète\n",
            "Progression : 95.24% complète\n",
            "Progression : 95.37% complète\n",
            "Progression : 95.49% complète\n",
            "Progression : 95.61% complète\n",
            "Progression : 95.73% complète\n",
            "Progression : 95.85% complète\n",
            "Progression : 95.98% complète\n",
            "Progression : 96.10% complète\n",
            "Progression : 96.22% complète\n",
            "Progression : 96.34% complète\n",
            "Progression : 96.46% complète\n",
            "Progression : 96.59% complète\n",
            "Progression : 96.71% complète\n",
            "Progression : 96.83% complète\n",
            "Progression : 96.95% complète\n",
            "Progression : 97.07% complète\n",
            "Progression : 97.20% complète\n",
            "Progression : 97.32% complète\n",
            "Progression : 97.44% complète\n",
            "Progression : 97.56% complète\n",
            "Progression : 97.68% complète\n",
            "Progression : 97.80% complète\n",
            "Progression : 97.93% complète\n",
            "Progression : 98.05% complète\n",
            "Progression : 98.17% complète\n",
            "Progression : 98.29% complète\n",
            "Progression : 98.41% complète\n",
            "Progression : 98.54% complète\n",
            "Progression : 98.66% complète\n",
            "Progression : 98.78% complète\n",
            "Progression : 98.90% complète\n",
            "Progression : 99.02% complète\n",
            "Progression : 99.15% complète\n",
            "Progression : 99.27% complète\n",
            "Progression : 99.39% complète\n",
            "Progression : 99.51% complète\n",
            "Progression : 99.63% complète\n",
            "Progression : 99.76% complète\n",
            "Progression : 99.88% complète\n",
            "Progression : 100.00% complète\n",
            "\n",
            "Tokenisation terminée.\n"
          ]
        }
      ],
      "source": [
        "# Transformation de chaque texte dans X_train en une séquence de float\n",
        "sequences_trainA,sequences_trainB,sequences_trainC = tokenizer.tokenize(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nombre de séquences de données: 1640\n"
          ]
        }
      ],
      "source": [
        "# Division des données en séquences de longueur maximale max_len\n",
        "data_trainA,y_train = diviser_liste(sequences_trainA,y_train, max_len)\n",
        "data_trainB = diviser_liste2(sequences_trainB, max_len)\n",
        "data_trainC = diviser_liste2(sequences_trainC, max_len)\n",
        "\n",
        "# Affichage de la longueur des données\n",
        "print(\"Nombre de séquences de données:\", len(data_trainA))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Save & Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "# np.save(\"Data/tokenized_train\",np.array(sequences_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "# sequences_train = np.load(\"Data/tokenized_train.npy\", allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# np.save(\"Data/data_split_train\",data_train)\n",
        "# np.save(\"Data/y_split_train\",y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# data_train = np.load(\"Data/data_split_train.npy\", allow_pickle=True)\n",
        "# y_train = np.load(\"Data/y_split_train.npy\", allow_pickle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Validation Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progression : 0.61% complète\n",
            "Progression : 1.23% complète\n",
            "Progression : 1.84% complète\n",
            "Progression : 2.45% complète\n",
            "Progression : 3.07% complète\n",
            "Progression : 3.68% complète\n",
            "Progression : 4.29% complète\n",
            "Progression : 4.91% complète\n",
            "Progression : 5.52% complète\n",
            "Progression : 6.13% complète\n",
            "Progression : 6.75% complète\n",
            "Progression : 7.36% complète\n",
            "Progression : 7.98% complète\n",
            "Progression : 8.59% complète\n",
            "Progression : 9.20% complète\n",
            "Progression : 9.82% complète\n",
            "Progression : 10.43% complète\n",
            "Progression : 11.04% complète\n",
            "Progression : 11.66% complète\n",
            "Progression : 12.27% complète\n",
            "Progression : 12.88% complète\n",
            "Progression : 13.50% complète\n",
            "Progression : 14.11% complète\n",
            "Progression : 14.72% complète\n",
            "Progression : 15.34% complète\n",
            "Progression : 15.95% complète\n",
            "Progression : 16.56% complète\n",
            "Progression : 17.18% complète\n",
            "Progression : 17.79% complète\n",
            "Progression : 18.40% complète\n",
            "Progression : 19.02% complète\n",
            "Progression : 19.63% complète\n",
            "Progression : 20.25% complète\n",
            "Progression : 20.86% complète\n",
            "Progression : 21.47% complète\n",
            "Progression : 22.09% complète\n",
            "Progression : 22.70% complète\n",
            "Progression : 23.31% complète\n",
            "Progression : 23.93% complète\n",
            "Progression : 24.54% complète\n",
            "Progression : 25.15% complète\n",
            "Progression : 25.77% complète\n",
            "Progression : 26.38% complète\n",
            "Progression : 26.99% complète\n",
            "Progression : 27.61% complète\n",
            "Progression : 28.22% complète\n",
            "Progression : 28.83% complète\n",
            "Progression : 29.45% complète\n",
            "Progression : 30.06% complète\n",
            "Progression : 30.67% complète\n",
            "Progression : 31.29% complète\n",
            "Progression : 31.90% complète\n",
            "Progression : 32.52% complète\n",
            "Progression : 33.13% complète\n",
            "Progression : 33.74% complète\n",
            "Progression : 34.36% complète\n",
            "Progression : 34.97% complète\n",
            "Progression : 35.58% complète\n",
            "Progression : 36.20% complète\n",
            "Progression : 36.81% complète\n",
            "Progression : 37.42% complète\n",
            "Progression : 38.04% complète\n",
            "Progression : 38.65% complète\n",
            "Progression : 39.26% complète\n",
            "Progression : 39.88% complète\n",
            "Progression : 40.49% complète\n",
            "Progression : 41.10% complète\n",
            "Progression : 41.72% complète\n",
            "Progression : 42.33% complète\n",
            "Progression : 42.94% complète\n",
            "Progression : 43.56% complète\n",
            "Progression : 44.17% complète\n",
            "Progression : 44.79% complète\n",
            "Progression : 45.40% complète\n",
            "Progression : 46.01% complète\n",
            "Progression : 46.63% complète\n",
            "Progression : 47.24% complète\n",
            "Progression : 47.85% complète\n",
            "Progression : 48.47% complète\n",
            "Progression : 49.08% complète\n",
            "Progression : 49.69% complète\n",
            "Progression : 50.31% complète\n",
            "Progression : 50.92% complète\n",
            "Progression : 51.53% complète\n",
            "Progression : 52.15% complète\n",
            "Progression : 52.76% complète\n",
            "Progression : 53.37% complète\n",
            "Progression : 53.99% complète\n",
            "Progression : 54.60% complète\n",
            "Progression : 55.21% complète\n",
            "Progression : 55.83% complète\n",
            "Progression : 56.44% complète\n",
            "Progression : 57.06% complète\n",
            "Progression : 57.67% complète\n",
            "Progression : 58.28% complète\n",
            "Progression : 58.90% complète\n",
            "Progression : 59.51% complète\n",
            "Progression : 60.12% complète\n",
            "Progression : 60.74% complète\n",
            "Progression : 61.35% complète\n",
            "Progression : 61.96% complète\n",
            "Progression : 62.58% complète\n",
            "Progression : 63.19% complète\n",
            "Progression : 63.80% complète\n",
            "Progression : 64.42% complète\n",
            "Progression : 65.03% complète\n",
            "Progression : 65.64% complète\n",
            "Progression : 66.26% complète\n",
            "Progression : 66.87% complète\n",
            "Progression : 67.48% complète\n",
            "Progression : 68.10% complète\n",
            "Progression : 68.71% complète\n",
            "Progression : 69.33% complète\n",
            "Progression : 69.94% complète\n",
            "Progression : 70.55% complète\n",
            "Progression : 71.17% complète\n",
            "Progression : 71.78% complète\n",
            "Progression : 72.39% complète\n",
            "Progression : 73.01% complète\n",
            "Progression : 73.62% complète\n",
            "Progression : 74.23% complète\n",
            "Progression : 74.85% complète\n",
            "Progression : 75.46% complète\n",
            "Progression : 76.07% complète\n",
            "Progression : 76.69% complète\n",
            "Progression : 77.30% complète\n",
            "Progression : 77.91% complète\n",
            "Progression : 78.53% complète\n",
            "Progression : 79.14% complète\n",
            "Progression : 79.75% complète\n",
            "Progression : 80.37% complète\n",
            "Progression : 80.98% complète\n",
            "Progression : 81.60% complète\n",
            "Progression : 82.21% complète\n",
            "Progression : 82.82% complète\n",
            "Progression : 83.44% complète\n",
            "Progression : 84.05% complète\n",
            "Progression : 84.66% complète\n",
            "Progression : 85.28% complète\n",
            "Progression : 85.89% complète\n",
            "Progression : 86.50% complète\n",
            "Progression : 87.12% complète\n",
            "Progression : 87.73% complète\n",
            "Progression : 88.34% complète\n",
            "Progression : 88.96% complète\n",
            "Progression : 89.57% complète\n",
            "Progression : 90.18% complète\n",
            "Progression : 90.80% complète\n",
            "Progression : 91.41% complète\n",
            "Progression : 92.02% complète\n",
            "Progression : 92.64% complète\n",
            "Progression : 93.25% complète\n",
            "Progression : 93.87% complète\n",
            "Progression : 94.48% complète\n",
            "Progression : 95.09% complète\n",
            "Progression : 95.71% complète\n",
            "Progression : 96.32% complète\n",
            "Progression : 96.93% complète\n",
            "Progression : 97.55% complète\n",
            "Progression : 98.16% complète\n",
            "Progression : 98.77% complète\n",
            "Progression : 99.39% complète\n",
            "Progression : 100.00% complète\n",
            "\n",
            "Tokenisation terminée.\n"
          ]
        }
      ],
      "source": [
        "sequences_valA,sequences_valB,sequences_valC = tokenizer.tokenize(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nombre de séquences de données: 334\n"
          ]
        }
      ],
      "source": [
        "# Division des données en séquences de longueur maximale max_len\n",
        "data_valA,y_val = diviser_liste(sequences_valA,y_val, max_len)\n",
        "data_valB = diviser_liste2(sequences_valB, max_len)\n",
        "data_valC =diviser_liste2(sequences_valC, max_len)\n",
        "# Affichage de la longueur des données\n",
        "print(\"Nombre de séquences de données:\", len(data_valA))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Save & Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "# np.save(\"Data/tokenized_val\",np.array(sequences_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "# sequences_val = np.load(\"Data/tokenized_val.npy\", allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "# np.save(\"Data/data_split_val\",data_val)\n",
        "# np.save(\"Data/y_split_val\",y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "# data_val = np.load(\"Data/data_split_val.npy\", allow_pickle=True)\n",
        "# y_val = np.load(\"Data/y_split_val.npy\", allow_pickle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progression : 1.45% complète\n",
            "Progression : 2.90% complète\n",
            "Progression : 4.35% complète\n",
            "Progression : 5.80% complète\n",
            "Progression : 7.25% complète\n",
            "Progression : 8.70% complète\n",
            "Progression : 10.14% complète\n",
            "Progression : 11.59% complète\n",
            "Progression : 13.04% complète\n",
            "Progression : 14.49% complète\n",
            "Progression : 15.94% complète\n",
            "Progression : 17.39% complète\n",
            "Progression : 18.84% complète\n",
            "Progression : 20.29% complète\n",
            "Progression : 21.74% complète\n",
            "Progression : 23.19% complète\n",
            "Progression : 24.64% complète\n",
            "Progression : 26.09% complète\n",
            "Progression : 27.54% complète\n",
            "Progression : 28.99% complète\n",
            "Progression : 30.43% complète\n",
            "Progression : 31.88% complète\n",
            "Progression : 33.33% complète\n",
            "Progression : 34.78% complète\n",
            "Progression : 36.23% complète\n",
            "Progression : 37.68% complète\n",
            "Progression : 39.13% complète\n",
            "Progression : 40.58% complète\n",
            "Progression : 42.03% complète\n",
            "Progression : 43.48% complète\n",
            "Progression : 44.93% complète\n",
            "Progression : 46.38% complète\n",
            "Progression : 47.83% complète\n",
            "Progression : 49.28% complète\n",
            "Progression : 50.72% complète\n",
            "Progression : 52.17% complète\n",
            "Progression : 53.62% complète\n",
            "Progression : 55.07% complète\n",
            "Progression : 56.52% complète\n",
            "Progression : 57.97% complète\n",
            "Progression : 59.42% complète\n",
            "Progression : 60.87% complète\n",
            "Progression : 62.32% complète\n",
            "Progression : 63.77% complète\n",
            "Progression : 65.22% complète\n",
            "Progression : 66.67% complète\n",
            "Progression : 68.12% complète\n",
            "Progression : 69.57% complète\n",
            "Progression : 71.01% complète\n",
            "Progression : 72.46% complète\n",
            "Progression : 73.91% complète\n",
            "Progression : 75.36% complète\n",
            "Progression : 76.81% complète\n",
            "Progression : 78.26% complète\n",
            "Progression : 79.71% complète\n",
            "Progression : 81.16% complète\n",
            "Progression : 82.61% complète\n",
            "Progression : 84.06% complète\n",
            "Progression : 85.51% complète\n",
            "Progression : 86.96% complète\n",
            "Progression : 88.41% complète\n",
            "Progression : 89.86% complète\n",
            "Progression : 91.30% complète\n",
            "Progression : 92.75% complète\n",
            "Progression : 94.20% complète\n",
            "Progression : 95.65% complète\n",
            "Progression : 97.10% complète\n",
            "Progression : 98.55% complète\n",
            "Progression : 100.00% complète\n",
            "\n",
            "Tokenisation terminée.\n"
          ]
        }
      ],
      "source": [
        "# Transformation de chaque texte dans X_test en une séquence d'entiers\n",
        "sequences_testA,sequences_testB,sequences_testC = tokenizer.tokenize(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nombre de séquences de données: 144\n"
          ]
        }
      ],
      "source": [
        "# Division des données en séquences de longueur maximale max_len\n",
        "data_testA,y_test = diviser_liste(sequences_testA,y_test, max_len)\n",
        "data_testB = diviser_liste2(sequences_testB, max_len)\n",
        "data_testC = diviser_liste2(sequences_testC, max_len)\n",
        "\n",
        "# Affichage de la longueur des données\n",
        "print(\"Nombre de séquences de données:\", len(data_testA))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Save & Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "# np.save(\"Data/tokenized_test\",np.array(sequences_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "# sequences_test = np.load(\"Data/tokenized_test.npy\", allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "# np.save(\"Data/data_split_test\",data_test)\n",
        "# np.save(\"Data/y_split_test\",y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "# data_test = np.load(\"Data/data_split_test.npy\", allow_pickle=True)\n",
        "# y_test = np.load(\"Data/y_split_test.npy\", allow_pickle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Affichage informations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nombre de séquences de données (Training): 1640\n",
            "Nombre de séquences de données (Validation): 334\n",
            "Nombre de séquences de données (Test): 144\n"
          ]
        }
      ],
      "source": [
        "print(\"Nombre de séquences de données (Training):\", len(data_trainA))\n",
        "print(\"Nombre de séquences de données (Validation):\", len(data_valA))\n",
        "print(\"Nombre de séquences de données (Test):\", len(data_testA))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nquwvt_dB3hx"
      },
      "source": [
        "# Définition des architectures des modeles testés"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "RGosKjmyzmuW"
      },
      "outputs": [],
      "source": [
        "from keras.models import Model\n",
        "\n",
        "def define_multi_model(max_len, vocab_size):\n",
        "    # define two sets of inputs\n",
        "    inputA = Input(shape=(max_len,))\n",
        "    inputB = Input(shape=(max_len,))\n",
        "    inputC = Input(shape=(max_len,))\n",
        "    \n",
        "    # add embedding layer to the first input\n",
        "    embedded_A = Embedding(input_dim=vocab_size, output_dim=10, input_length=max_len)(inputA)\n",
        "    # add Conv1D layer to the embedded input\n",
        "    conv1d_A = Conv1D(filters=64, kernel_size=81, activation='relu')(embedded_A)\n",
        "    pool_A = MaxPooling1D(pool_size=2)(conv1d_A)\n",
        "    conv1d_A2 = Conv1D(filters=64, kernel_size=81, activation='relu')(pool_A)\n",
        "    pool_A2 = MaxPooling1D(pool_size=2)(conv1d_A2)\n",
        "    conv1d_A3 = Conv1D(filters=64, kernel_size=81, activation='relu')(pool_A2)\n",
        "    pool_A3 = MaxPooling1D(pool_size=2)(conv1d_A3)\n",
        "    # create a model for the first branch\n",
        "    #x = Dense(128, activation=\"relu\")(pool_A)\n",
        "    model_A = Model(inputs=inputA, outputs=pool_A3)\n",
        "    \n",
        "    # add embedding layer to the second input\n",
        "    embedded_B = Embedding(input_dim=vocab_size, output_dim=10, input_length=max_len)(inputB)\n",
        "    # add Conv1D layer to the embedded input\n",
        "    conv1d_B = Conv1D(filters=64, kernel_size=81, activation='relu')(embedded_B)\n",
        "    pool_B = MaxPooling1D(pool_size=2)(conv1d_B)\n",
        "    conv1d_B2 = Conv1D(filters=64, kernel_size=81, activation='relu')(pool_B)\n",
        "    pool_B2 = MaxPooling1D(pool_size=2)(conv1d_B2)\n",
        "    conv1d_B3 = Conv1D(filters=64, kernel_size=81, activation='relu')(pool_B2)\n",
        "    pool_B3 = MaxPooling1D(pool_size=2)(conv1d_B3)\n",
        "    # create a model for the second branch\n",
        "    #y = Dense(128, activation=\"relu\")(pool_B)\n",
        "    model_B = Model(inputs=inputB, outputs=pool_B3)\n",
        "\n",
        "    # add embedding layer to the third input\n",
        "    embedded_C = Embedding(input_dim=vocab_size, output_dim=10, input_length=max_len)(inputC)\n",
        "    # add Conv1D layer to the embedded input\n",
        "    conv1d_C = Conv1D(filters=64, kernel_size=81, activation='relu')(embedded_C)\n",
        "    pool_C = MaxPooling1D(pool_size=2)(conv1d_C)\n",
        "    conv1d_C2 = Conv1D(filters=64, kernel_size=81, activation='relu')(pool_C)\n",
        "    pool_C2 = MaxPooling1D(pool_size=2)(conv1d_C2)\n",
        "    conv1d_C3 = Conv1D(filters=64, kernel_size=81, activation='relu')(pool_C2)\n",
        "    pool_C3 = MaxPooling1D(pool_size=2)(conv1d_C3)\n",
        "    # create a model for the first branch\n",
        "    #x = Dense(128, activation=\"relu\")(pool_A)\n",
        "    model_C = Model(inputs=inputC, outputs=pool_C3)\n",
        "    \n",
        "    # combine the output of the two branches\n",
        "    combined = concatenate([model_A.output, model_B.output])\n",
        "    combined = concatenate([model_C.output, combined])\n",
        "    # apply a FC layer and then a regression prediction on the\n",
        "    # combined outputs\n",
        "    z = Flatten()(combined)\n",
        "    z = Dense(64, activation=\"relu\")(z)\n",
        "    z = Dense(1, activation=\"linear\")(z)\n",
        "    # our model will accept the inputs of the two branches and\n",
        "    # then output a single value\n",
        "    model = Model(inputs=[inputA, inputB, inputC], outputs=z)\n",
        "    return model\n",
        "\n",
        "def archi(model_params,input_shape):\n",
        "    # Pile linéaire de couches. \n",
        "    # Dans ce modèle, les données passent à travers les couches dans l'ordre où elles ont été ajoutées.\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Input(shape=input_shape))\n",
        "\n",
        "    model.add(Embedding(input_dim=token_number, output_dim=model_params['embedding_dim']))\n",
        "\n",
        "    model.add(Reshape(target_shape=(max_len, model_params['embedding_dim'] * input_shape[1])))\n",
        "\n",
        "    for i in range(model_params['count_conv1D']):\n",
        "        # Couche de convolution à une dimension (1D) au modèle.\n",
        "        model.add(Conv1D(filters=model_params['conv1D_1_filters'], kernel_size=model_params['conv1D_1_kernel'], activation='relu', kernel_regularizer=regularizers.l2(l=0.001)))\n",
        "\n",
        "        if(model_params['pool_size'] != 0):\n",
        "            # Réduction de dimension par l'opération de pooling.\n",
        "            model.add(MaxPooling1D(pool_size=model_params['pool_size']))\n",
        "\n",
        "    if(model_params['dense_units'] != 0):\n",
        "        # Couche dense de neurones avec activation ReLU et régularisation L2.\n",
        "        model.add(Dense(units=model_params['dense_units'],kernel_regularizer=regularizers.l2(l=0.001)))\n",
        "\n",
        "    if(model_params['dropout'] != 0):\n",
        "        # Dropout pour éviter le surapprentissage.\n",
        "        model.add(Dropout(rate=model_params['dropout']))\n",
        "\n",
        "    # Transformation des données en un format adapté à une couche Dense.\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # Couche de sortie avec une seule sortie.\n",
        "    model.add(Dense(units=1))\n",
        "\n",
        "    return model\n",
        "\n",
        "# La fonction ReLU est couramment utilisée dans les réseaux de neurones en raison de sa simplicité et de sa capacité à introduire une non-linéarité dans le modèle,\n",
        "# ce qui permet au réseau de capturer des motifs complexes dans les données."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tofi22Hb2KQQ"
      },
      "source": [
        "# Entrainement des models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIvj5MS82Q2D"
      },
      "source": [
        "## Ou en entrainer un seul"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Un seul"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwx9k9Nc3Alv",
        "outputId": "4429e182-83ce-41d3-b151-a65d57bf6183"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\jiang\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
            "\n",
            "WARNING:tensorflow:From C:\\Users\\jiang\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\backend.py:6642: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From C:\\Users\\jiang\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 30000)]              0         []                            \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, 30000)]              0         []                            \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)        [(None, 30000)]              0         []                            \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 30000, 10)            2913900   ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)     (None, 30000, 10)            2913900   ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " embedding_2 (Embedding)     (None, 30000, 10)            2913900   ['input_3[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)             (None, 29920, 64)            51904     ['embedding[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)           (None, 29920, 64)            51904     ['embedding_1[0][0]']         \n",
            "                                                                                                  \n",
            " conv1d_6 (Conv1D)           (None, 29920, 64)            51904     ['embedding_2[0][0]']         \n",
            "                                                                                                  \n",
            " max_pooling1d (MaxPooling1  (None, 14960, 64)            0         ['conv1d[0][0]']              \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " max_pooling1d_3 (MaxPoolin  (None, 14960, 64)            0         ['conv1d_3[0][0]']            \n",
            " g1D)                                                                                             \n",
            "                                                                                                  \n",
            " max_pooling1d_6 (MaxPoolin  (None, 14960, 64)            0         ['conv1d_6[0][0]']            \n",
            " g1D)                                                                                             \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)           (None, 14880, 64)            331840    ['max_pooling1d[0][0]']       \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)           (None, 14880, 64)            331840    ['max_pooling1d_3[0][0]']     \n",
            "                                                                                                  \n",
            " conv1d_7 (Conv1D)           (None, 14880, 64)            331840    ['max_pooling1d_6[0][0]']     \n",
            "                                                                                                  \n",
            " max_pooling1d_1 (MaxPoolin  (None, 7440, 64)             0         ['conv1d_1[0][0]']            \n",
            " g1D)                                                                                             \n",
            "                                                                                                  \n",
            " max_pooling1d_4 (MaxPoolin  (None, 7440, 64)             0         ['conv1d_4[0][0]']            \n",
            " g1D)                                                                                             \n",
            "                                                                                                  \n",
            " max_pooling1d_7 (MaxPoolin  (None, 7440, 64)             0         ['conv1d_7[0][0]']            \n",
            " g1D)                                                                                             \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)           (None, 7360, 64)             331840    ['max_pooling1d_1[0][0]']     \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)           (None, 7360, 64)             331840    ['max_pooling1d_4[0][0]']     \n",
            "                                                                                                  \n",
            " conv1d_8 (Conv1D)           (None, 7360, 64)             331840    ['max_pooling1d_7[0][0]']     \n",
            "                                                                                                  \n",
            " max_pooling1d_2 (MaxPoolin  (None, 3680, 64)             0         ['conv1d_2[0][0]']            \n",
            " g1D)                                                                                             \n",
            "                                                                                                  \n",
            " max_pooling1d_5 (MaxPoolin  (None, 3680, 64)             0         ['conv1d_5[0][0]']            \n",
            " g1D)                                                                                             \n",
            "                                                                                                  \n",
            " max_pooling1d_8 (MaxPoolin  (None, 3680, 64)             0         ['conv1d_8[0][0]']            \n",
            " g1D)                                                                                             \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 3680, 128)            0         ['max_pooling1d_2[0][0]',     \n",
            "                                                                     'max_pooling1d_5[0][0]']     \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate  (None, 3680, 192)            0         ['max_pooling1d_8[0][0]',     \n",
            " )                                                                   'concatenate[0][0]']         \n",
            "                                                                                                  \n",
            " flatten (Flatten)           (None, 706560)               0         ['concatenate_1[0][0]']       \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 64)                   4521990   ['flatten[0][0]']             \n",
            "                                                          4                                       \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 1)                    65        ['dense[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 56108421 (214.04 MB)\n",
            "Trainable params: 56108421 (214.04 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/200\n",
            "WARNING:tensorflow:From C:\\Users\\jiang\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
            "\n",
            "WARNING:tensorflow:From C:\\Users\\jiang\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
            "\n",
            " 2/26 [=>............................] - ETA: 20:04 - loss: 3490431.2500 - mae: 1865.4971"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[50], line 41\u001b[0m\n\u001b[0;32m     30\u001b[0m test \u001b[38;5;241m=\u001b[39m [data_testA,data_testB,data_testC]\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# with tf.device('/cpu:0'):\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m#    data_train_tensor = tf.convert_to_tensor(data_train, np.float32)\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m#    y_train_tensor = tf.convert_to_tensor(y_train, np.float32)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m \n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Entraînement du modèle avec les données d'entraînement et de validation, en utilisant EarlyStopping pour éviter le surapprentissage\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_val\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# L'historique de l'entraînement\u001b[39;00m\n\u001b[0;32m     44\u001b[0m history\u001b[38;5;241m.\u001b[39mhistory\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\engine\\training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1805\u001b[0m ):\n\u001b[0;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m     args,\n\u001b[0;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1327\u001b[0m     executing_eagerly)\n\u001b[0;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1501\u001b[0m   )\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "utils.set_random_seed(420)\n",
        "\n",
        "model_params = {'embedding_dim': 15,\n",
        "                'count_conv1D':1,\n",
        "                'conv1D_1_filters': 64,\n",
        "                'conv1D_1_kernel': 9,\n",
        "                'pool_size':2,\n",
        "                'dense_units': 32,\n",
        "                'dropout':0.4 }\n",
        "\n",
        "# Création du modèle en utilisant l'architecture spécifiée\n",
        "input_shape = (max_len, embedding_dim+word_param_number)\n",
        "#model = archi(model_params,input_shape)\n",
        "model = define_multi_model(max_len, token_number)\n",
        "\n",
        "\n",
        "# Compilation du modèle avec l'optimiseur Adam, la fonction de perte 'mse' et la métrique 'mae'\n",
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
        "# Affichage de la structure du modèle\n",
        "model.summary()\n",
        "\n",
        "# Création du callback EarlyStopping pour surveiller la perte d'entraînement\n",
        "early_stopping = EarlyStopping(monitor='loss', patience=3)\n",
        "\n",
        "# Création du callback EarlyStopping pour surveiller la perte de validation\n",
        "early_stopping_val = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "train = [data_trainA,data_trainB,data_trainC]\n",
        "val = [data_valA,data_valB,data_valC]\n",
        "test = [data_testA,data_testB,data_testC]\n",
        "\n",
        "# with tf.device('/cpu:0'):\n",
        "#    data_train_tensor = tf.convert_to_tensor(data_train, np.float32)\n",
        "#    y_train_tensor = tf.convert_to_tensor(y_train, np.float32)\n",
        "#    data_val_tensor = tf.convert_to_tensor(data_val, np.float32)\n",
        "#    y_val_tensor = tf.convert_to_tensor(y_val, np.float32)\n",
        "#    data_test_tensor = tf.convert_to_tensor(data_test, np.float32)\n",
        "#    y_test_tensor = tf.convert_to_tensor(y_test, np.float32)\n",
        "\n",
        "# Entraînement du modèle avec les données d'entraînement et de validation, en utilisant EarlyStopping pour éviter le surapprentissage\n",
        "history = model.fit(train, y_train, epochs=200, batch_size=64, validation_data=(val, y_val), callbacks=[early_stopping, early_stopping_val])\n",
        "\n",
        "# L'historique de l'entraînement\n",
        "history.history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation du model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the model on the test data\n",
        "print(\"Evaluation on Test Data:\")\n",
        "loss_test,mae_test = model.evaluate(test, y_test)\n",
        "print()\n",
        "\n",
        "# Evaluate the model on the training data\n",
        "print(\"Evaluation on Training Data:\")\n",
        "loss_train,mae_train = model.evaluate(train, y_train)\n",
        "print()\n",
        "\n",
        "# Evaluate the model on the validation data\n",
        "print(\"Evaluation on Validation Data:\")\n",
        "loss_validation,mae_validation = model.evaluate(val, y_val)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = model.predict(test)\n",
        "\n",
        "error_samples = []\n",
        "tolerance = 15\n",
        "within_tolerance_count = 0\n",
        "\n",
        "for i in range(len(y_test)):\n",
        "    if abs(round(y_pred[i][0]) - y_test[i]) <= tolerance:\n",
        "        within_tolerance_count += 1\n",
        "\n",
        "accuracy_within_tolerance = (within_tolerance_count / len(y_test)) * 100\n",
        "\n",
        "print(\"Accuracy within tolerance:\", accuracy_within_tolerance, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Save in Datas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# df_models = pd.DataFrame()\n",
        "\n",
        "# df_models['Model Name'] = [\"Model 1\"]\n",
        "# df_models['MAE'] = [mae_test]\n",
        "# df_models['MSE'] = [loss_test]\n",
        "# df_models['Accuracy with tolerance 15 (%)'] = [accuracy_within_tolerance]\n",
        "# # df_models['Train_MAE'] = [mae_train]\n",
        "# # df_models['Train_MSE'] = [loss_train]\n",
        "# # df_models['Val_MAE'] = [mae_validation]\n",
        "# # df_models['Val_MSE'] = [loss_validation]\n",
        "\n",
        "# # Add columns for each element in model_params\n",
        "# for key, value in model_params.items():\n",
        "#     df_models[key] = value\n",
        "\n",
        "# df_models.to_csv('df_models.csv', index=False)\n",
        "\n",
        "# # Afficher les données\n",
        "# display(df_models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Add data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Pour supprimer une mauvaise ligne\n",
        "# # Charger les données à partir du fichier CSV\n",
        "# df_models = pd.read_csv('df_models.csv')\n",
        "# df_models.drop(len(df_models)-1, inplace=True)\n",
        "# display(df_models)\n",
        "# df_models.to_csv('df_models.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Charger les données à partir du fichier CSV\n",
        "df_models = pd.read_csv('df_models.csv')\n",
        "model_name = \"Model \"+str(len(df_models)+1)\n",
        "nouvelle_ligne = [model_name,\n",
        "                  mae_test,\n",
        "                  loss_test,\n",
        "                  accuracy_within_tolerance,\n",
        "                  model_params['embedding_dim'],\n",
        "                  model_params['conv1D_1_filters'],\n",
        "                  model_params['conv1D_1_kernel'],\n",
        "                  model_params['pool_size'],\n",
        "                  model_params['dense_units'],\n",
        "                  model_params['dropout']]\n",
        "\n",
        "# Ajouter la nouvelle ligne à la DataFrame\n",
        "df_models.loc[len(df_models)] = nouvelle_ligne\n",
        "\n",
        "df_models.to_csv('df_models.csv', index=False)\n",
        "\n",
        "# Afficher les données mises à jour\n",
        "display(df_models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utiliser le modèle pour prédire les dates sur les données de test\n",
        "predictions = model.predict(test)\n",
        "\n",
        "# Afficher les dates prédites\n",
        "print(\"Dates prédites :\")\n",
        "for prediction in predictions:\n",
        "    print(prediction)\n",
        "\n",
        "print(\"max: \",max(predictions))\n",
        "print(\"min: \",min(predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Graphes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'][1:])\n",
        "plt.plot(history.history['val_loss'][1:])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "#Plot MAE\n",
        "plt.plot(history.history['mae'][1:]) \n",
        "plt.plot(history.history['val_mae'][1:]) \n",
        "plt.title('Model Mean Absolute Error') \n",
        "plt.ylabel('MAE') \n",
        "plt.xlabel('Epoch') \n",
        "plt.legend(['Train', 'Validation'], loc='upper right') \n",
        "plt.show()\n",
        "\n",
        "#Calcul de la différence entre les années prédites et les années réelles\n",
        "y_pred = model.predict(test) \n",
        "diff = np.abs(y_test - y_pred.flatten())\n",
        "\n",
        "#le dernier bin\n",
        "max_diff = np.max(diff)\n",
        "\n",
        "#Plot de l'histogramme des différences entre les années prédites et réelles\n",
        "plt.hist(diff, bins=np.arange(0,max_diff,1)) \n",
        "plt.xlabel('Difference entre années prédites et réelles') \n",
        "plt.ylabel('Nombre de prédictions') \n",
        "plt.title('Répartition des différences entre les années prédites et réelles') \n",
        "plt.show()\n",
        "\n",
        "\n",
        "#-----------------------------------voir quelle année est la plus dure----------------------------------#\n",
        "y_pred = model.predict(test)\n",
        "error_samples = []\n",
        "exact_matches_count = 0\n",
        "\n",
        "for i in range(len(y_test)):\n",
        "    if round(y_pred[i][0]) != y_test[i]:\n",
        "        error_samples.append(y_test[i])\n",
        "    else:\n",
        "        exact_matches_count += 1\n",
        "\n",
        "accuracy_exact_matches = (exact_matches_count / len(y_test)) * 100\n",
        "\n",
        "print(\"Accuracy avec exacte matching:\", accuracy_exact_matches, \"%\")\n",
        "\n",
        "\n",
        "#------------------------voir quelle année est la plus dure (tolerance de 15)---------------------------#\n",
        "error_samples = []\n",
        "tolerance = 15\n",
        "within_tolerance_count = 0\n",
        "\n",
        "for i in range(len(y_test)):\n",
        "    if abs(round(y_pred[i][0]) - y_test[i]) > tolerance:\n",
        "        error_samples.append(y_test[i])\n",
        "    else:\n",
        "        within_tolerance_count += 1\n",
        "\n",
        "accuracy_within_tolerance = (within_tolerance_count / len(y_test)) * 100\n",
        "\n",
        "print(\"Accuracy avec tolerance de\", tolerance, \"est:\", accuracy_within_tolerance, \"%\")\n",
        "\n",
        "\n",
        "# Créer un histogramme des années d'erreur\n",
        "plt.hist(error_samples, bins=max(y_test)-min(y_test)+1, color='skyblue')\n",
        "plt.xlabel('Année')\n",
        "plt.ylabel('Nombre d\\'échantillons mal prédits')\n",
        "plt.title('Histogramme des années d\\'erreur')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load & Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# #Sauvegarder le modèle complet au format HDF5\n",
        "# model.save('model/modelK81.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Charger le modèle complet\n",
        "# from keras.models import load_model\n",
        "# model = load_model('modelV4.h5')\n",
        "\n",
        "# model.summary()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "5CK1OPbMz2vm",
        "bIvj5MS82Q2D",
        "s71plxAN2fbA"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
