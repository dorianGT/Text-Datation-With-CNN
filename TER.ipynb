{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CK1OPbMz2vm"
      },
      "source": [
        "# Util Collab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !python -m pip install tensorflow keras\n",
        "# !python -m pip install scikit-learn\n",
        "# !python -m pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "r13TuBJFtvXQ"
      },
      "outputs": [],
      "source": [
        "# !pip install tensorflow==2.8.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "BQAuZrHDwSLL"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade tensorflow scikeras keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qgV6HWc0CBC"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "Ql2XMCihytNu"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\doria\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, recall_score, f1_score, matthews_corrcoef, roc_auc_score\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Flatten,BatchNormalization\n",
        "from keras import regularizers\n",
        "\n",
        "import re\n",
        "import os\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aE7LmOth00nu"
      },
      "source": [
        "# Coder le texte"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HyXdOTf0H4c"
      },
      "source": [
        "## Récupération des textes et des années de publications (labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shbqFxbaboYN",
        "outputId": "e204a37b-37e5-401c-e6be-fd2aa84d648b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1832, 1920, 1954, 1985, 1870, 1923, 1866, 1986, 1933, 2005, 1897, 1987, 1882, 2010, 1962, 1910, 1908, 1895, 1830, 2016, 2015, 1989, 1977, 1967, 1964, 1909, 1996, 1907, 2013, 1920, 1949, 1958, 1988, 1997, 1954, 1912, 1990, 1948, 1934, 1997, 1908, 1949, 1995, 2002, 2014, 2005, 1898, 1927, 2019, 1843, 1970, 1992, 1925, 1695, 2010, 2014, 1978, 1962, 2011, 1963, 1833, 1927, 1887, 1923, 1921, 1979, 2005, 1964, 2017, 2006, 1990, 2013, 1852, 1874, 1900, 1971, 2015, 1941, 1902, 2016, 1975, 2014, 1893, 1943, 1905, 1888, 2001, 2009, 1833, 2004, 1901, 1903, 1937, 1957, 1895, 1998, 2015, 2013, 1862, 1920, 1916, 1860, 1892, 1772, 1926, 1965, 2003, 2002, 1964, 1968, 1917, 1994, 1788, 1944, 1857, 1947, 1924, 1929, 1983, 1922, 2019, 2006, 1867, 1911, 1940, 1978, 1985, 1929, 1967, 1884, 1962, 2000, 1897, 1948, 1991, 1999, 1937, 1854, 1949, 2014, 1856, 2002, 1856, 1902, 1963, 1968, 1902, 1888, 2014, 1834, 1926, 2011, 2008, 2004, 1865, 2005, 1874, 2011, 2009, 1945, 2013, 1924, 1960, 2017, 2002, 2007, 1963, 1988, 1946, 1927, 1882, 1913, 2013, 2009, 1953, 1996, 1963, 1952, 1888, 1636, 1994, 2008, 1992, 1827, 1951, 1892, 1837, 2003, 1911, 1800, 1970, 1937, 2011, 2017, 1895, 1943, 1951, 1882, 1854, 1814, 1995, 1904, 1999, 2007, 1967, 1995, 1910, 1941, 1919, 1846, 2021, 2008, 2004, 1928, 2001, 1993, 1900, 1955, 1953, 1894, 1861, 1986, 1972, 1966, 1937, 1959, 1951, 1858, 2012, 1929, 1930, 1869, 2010, 1831, 1920, 1912, 1961, 2006, 1977, 1893, 1955, 1921, 1886, 1973, 2007, 1863, 2010, 2019, 2000, 2001, 1887, 1928, 1500, 1962, 1934, 1942, 1899, 1924, 2012, 1891, 2009, 1970, 1985, 2016, 1958, 1894, 1860, 1859, 1947, 1893, 1947, 1952, 1874, 2006, 1965, 1899, 1986, 1935, 1894, 1839, 1968, 1885, 1944, 1951, 1915, 1948, 1949, 1944, 2001, 1956, 1998, 1948, 1995, 1991, 2014, 2006, 2003, 2017, 1869, 1997, 1923, 1884, 2004, 2022, 1955, 1940, 2002, 1932, 1946, 1860, 1637, 1958, 1942, 2013, 2011, 1984, 1943, 1858, 1972, 1921, 1870, 1890, 1941, 1981, 1981, 2018, 1881, 1933, 1978, 1862, 2010, 1937, 2009, 1905, 1903, 1859, 2015, 2012, 1922, 1926, 1899, 1975, 2002, 1865, 1933, 2007, 1883, 1933, 2000, 1842, 1887, 1997, 1969, 1872, 1934, 1982, 2018, 1843, 2004, 2001, 1776, 1942, 1917, 2021, 1930, 1923, 1869, 1945, 1874, 1907, 1953, 1928, 1945, 1928, 1997, 1941, 1972, 1914, 1936, 1919, 1957, 1894, 1956, 1971, 1969, 1993, 1956, 1863, 1926, 1918, 1974, 1830, 2009, 1930, 1882, 1831, 1966, 1952, 2006, 1847, 2015, 1936, 1975, 1960, 2007]\n"
          ]
        }
      ],
      "source": [
        "# Chemin du dossier contenant les textes\n",
        "dossier_textes = 'CORPUS DATATION TER'\n",
        "\n",
        "# Récupérer la liste des noms de fichiers dans le dossier\n",
        "fichiers = os.listdir(dossier_textes)\n",
        "\n",
        "# Trier les fichiers par ordre alphabétique\n",
        "fichiers.sort()\n",
        "\n",
        "# Initialiser une liste pour stocker les textes\n",
        "books = []\n",
        "\n",
        "# Initialiser une liste pour stocker les années extraites des noms de fichiers\n",
        "y = []\n",
        "\n",
        "# Parcourir les fichiers et lire leur contenu\n",
        "for fichier in fichiers[:500]:\n",
        "    chemin_fichier = os.path.join(dossier_textes, fichier)\n",
        "    with open(chemin_fichier, 'r', encoding='utf-8') as f:\n",
        "\n",
        "        texte = f.read()\n",
        "\n",
        "        # Extraire l'année du nom de fichier\n",
        "        annee = re.search(r'\\((\\d{4})\\)', fichier)  # Utilisation d'une expression régulière pour trouver l'année entre parenthèses\n",
        "        # Si une année est trouvée, alors nous pouvons étudier le livre, nous l'ajoutons ainsi que son label dans les listes associées\n",
        "        if annee:\n",
        "            books.append(texte)\n",
        "            annee_int = int(annee.group(1)) # Récupérer l'année\n",
        "            y.append(annee_int)  # Ajouter l'année extraite à la liste des labels\n",
        "\n",
        "# Division de notre bdd en deux bdd, l'une pour l'entrainement et l'autre pour les tests\n",
        "X_train, X_test, y_train, y_test = train_test_split(books, y, test_size=0.1, random_state=42)\n",
        "X_train,X_val,y_train,y_val =  train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
        "\n",
        "# Afficher les annéees extraites\n",
        "print(y_train)\n",
        "#print(len(X_train[9]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qr1HZfr0gkd"
      },
      "source": [
        "## Tokenization + padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "OC9aLNIY7hMS"
      },
      "outputs": [],
      "source": [
        "# Paramètres du Tokenizer\n",
        "max_len = 5000 # Longueur maximale des séquences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "_7PvkrqXQRxl"
      },
      "outputs": [],
      "source": [
        "class Tokenizer2:\n",
        "    def __init__(self):\n",
        "        self.word_to_number = {}\n",
        "        self.next_number = 0\n",
        "\n",
        "    def tokenize(self, list_of_texts):\n",
        "        total_texts = len(list_of_texts)\n",
        "        token_lists = []\n",
        "        for i, text in enumerate(list_of_texts):\n",
        "            text = text.lower()\n",
        "            tokens = word_tokenize(text)\n",
        "            token_lists.append([])\n",
        "\n",
        "            for token in tokens:\n",
        "                if token not in self.word_to_number:\n",
        "                    self.word_to_number[token] = self.next_number\n",
        "                    self.next_number += 1\n",
        "\n",
        "                token_lists[-1].append(self.word_to_number[token])\n",
        "\n",
        "            percentage = (i + 1) / total_texts * 100\n",
        "            print(f\"Progress: {percentage:.2f}% complete\")\n",
        "\n",
        "        print(\"\\nTokenization complete.\")\n",
        "        return token_lists, self.next_number\n",
        "    \n",
        "    def tokenizeNoAdd(self, list_of_texts):\n",
        "        total_texts = len(list_of_texts)\n",
        "        token_lists = []\n",
        "        for i, text in enumerate(list_of_texts):\n",
        "            text = text.lower()\n",
        "            tokens = word_tokenize(text)\n",
        "            token_lists.append([])\n",
        "\n",
        "            for token in tokens:\n",
        "                if token not in self.word_to_number:\n",
        "                    continue\n",
        "\n",
        "                token_lists[-1].append(self.word_to_number[token])\n",
        "\n",
        "            percentage = (i + 1) / total_texts * 100\n",
        "            print(f\"Progress: {percentage:.2f}% complete\")\n",
        "\n",
        "        print(\"\\nTokenization complete.\")\n",
        "        return token_lists, self.next_number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "zj_poLDZfP0F"
      },
      "outputs": [],
      "source": [
        "def diviser_liste(liste,labels, taille):\n",
        "    resultats = []\n",
        "    res_labels = []\n",
        "    ind = 0\n",
        "    # Parcours de chaque sous-liste dans la liste principale\n",
        "    for sous_liste in liste:\n",
        "        longueur = len(sous_liste)\n",
        "        if(longueur<taille):\n",
        "            ind+=1\n",
        "            continue\n",
        "        nbtour = round(longueur/taille)\n",
        "        lastpos=0\n",
        "        # Division de la sous-liste en morceaux de taille spécifiée\n",
        "        for i in range(nbtour):\n",
        "            if(lastpos + taille > longueur):\n",
        "              lastpos -= (lastpos + taille)-longueur\n",
        "\n",
        "            # Ajout du morceau à la liste de résultats\n",
        "            resultat = sous_liste[lastpos:lastpos + taille]\n",
        "            lastpos = lastpos + taille\n",
        "            resultats.append(resultat)\n",
        "\n",
        "            # A chaque fois que je divisie, jajoute le label au meme indice\n",
        "            res_labels.append(labels[ind])\n",
        "        ind+=1\n",
        "\n",
        "    return resultats,res_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "NwFu_20zSujU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 0.25% complete\n",
            "Progress: 0.49% complete\n",
            "Progress: 0.74% complete\n",
            "Progress: 0.99% complete\n",
            "Progress: 1.23% complete\n",
            "Progress: 1.48% complete\n",
            "Progress: 1.73% complete\n",
            "Progress: 1.98% complete\n",
            "Progress: 2.22% complete\n",
            "Progress: 2.47% complete\n",
            "Progress: 2.72% complete\n",
            "Progress: 2.96% complete\n",
            "Progress: 3.21% complete\n",
            "Progress: 3.46% complete\n",
            "Progress: 3.70% complete\n",
            "Progress: 3.95% complete\n",
            "Progress: 4.20% complete\n",
            "Progress: 4.44% complete\n",
            "Progress: 4.69% complete\n",
            "Progress: 4.94% complete\n",
            "Progress: 5.19% complete\n",
            "Progress: 5.43% complete\n",
            "Progress: 5.68% complete\n",
            "Progress: 5.93% complete\n",
            "Progress: 6.17% complete\n",
            "Progress: 6.42% complete\n",
            "Progress: 6.67% complete\n",
            "Progress: 6.91% complete\n",
            "Progress: 7.16% complete\n",
            "Progress: 7.41% complete\n",
            "Progress: 7.65% complete\n",
            "Progress: 7.90% complete\n",
            "Progress: 8.15% complete\n",
            "Progress: 8.40% complete\n",
            "Progress: 8.64% complete\n",
            "Progress: 8.89% complete\n",
            "Progress: 9.14% complete\n",
            "Progress: 9.38% complete\n",
            "Progress: 9.63% complete\n",
            "Progress: 9.88% complete\n",
            "Progress: 10.12% complete\n",
            "Progress: 10.37% complete\n",
            "Progress: 10.62% complete\n",
            "Progress: 10.86% complete\n",
            "Progress: 11.11% complete\n",
            "Progress: 11.36% complete\n",
            "Progress: 11.60% complete\n",
            "Progress: 11.85% complete\n",
            "Progress: 12.10% complete\n",
            "Progress: 12.35% complete\n",
            "Progress: 12.59% complete\n",
            "Progress: 12.84% complete\n",
            "Progress: 13.09% complete\n",
            "Progress: 13.33% complete\n",
            "Progress: 13.58% complete\n",
            "Progress: 13.83% complete\n",
            "Progress: 14.07% complete\n",
            "Progress: 14.32% complete\n",
            "Progress: 14.57% complete\n",
            "Progress: 14.81% complete\n",
            "Progress: 15.06% complete\n",
            "Progress: 15.31% complete\n",
            "Progress: 15.56% complete\n",
            "Progress: 15.80% complete\n",
            "Progress: 16.05% complete\n",
            "Progress: 16.30% complete\n",
            "Progress: 16.54% complete\n",
            "Progress: 16.79% complete\n",
            "Progress: 17.04% complete\n",
            "Progress: 17.28% complete\n",
            "Progress: 17.53% complete\n",
            "Progress: 17.78% complete\n",
            "Progress: 18.02% complete\n",
            "Progress: 18.27% complete\n",
            "Progress: 18.52% complete\n",
            "Progress: 18.77% complete\n",
            "Progress: 19.01% complete\n",
            "Progress: 19.26% complete\n",
            "Progress: 19.51% complete\n",
            "Progress: 19.75% complete\n",
            "Progress: 20.00% complete\n",
            "Progress: 20.25% complete\n",
            "Progress: 20.49% complete\n",
            "Progress: 20.74% complete\n",
            "Progress: 20.99% complete\n",
            "Progress: 21.23% complete\n",
            "Progress: 21.48% complete\n",
            "Progress: 21.73% complete\n",
            "Progress: 21.98% complete\n",
            "Progress: 22.22% complete\n",
            "Progress: 22.47% complete\n",
            "Progress: 22.72% complete\n",
            "Progress: 22.96% complete\n",
            "Progress: 23.21% complete\n",
            "Progress: 23.46% complete\n",
            "Progress: 23.70% complete\n",
            "Progress: 23.95% complete\n",
            "Progress: 24.20% complete\n",
            "Progress: 24.44% complete\n",
            "Progress: 24.69% complete\n",
            "Progress: 24.94% complete\n",
            "Progress: 25.19% complete\n",
            "Progress: 25.43% complete\n",
            "Progress: 25.68% complete\n",
            "Progress: 25.93% complete\n",
            "Progress: 26.17% complete\n",
            "Progress: 26.42% complete\n",
            "Progress: 26.67% complete\n",
            "Progress: 26.91% complete\n",
            "Progress: 27.16% complete\n",
            "Progress: 27.41% complete\n",
            "Progress: 27.65% complete\n",
            "Progress: 27.90% complete\n",
            "Progress: 28.15% complete\n",
            "Progress: 28.40% complete\n",
            "Progress: 28.64% complete\n",
            "Progress: 28.89% complete\n",
            "Progress: 29.14% complete\n",
            "Progress: 29.38% complete\n",
            "Progress: 29.63% complete\n",
            "Progress: 29.88% complete\n",
            "Progress: 30.12% complete\n",
            "Progress: 30.37% complete\n",
            "Progress: 30.62% complete\n",
            "Progress: 30.86% complete\n",
            "Progress: 31.11% complete\n",
            "Progress: 31.36% complete\n",
            "Progress: 31.60% complete\n",
            "Progress: 31.85% complete\n",
            "Progress: 32.10% complete\n",
            "Progress: 32.35% complete\n",
            "Progress: 32.59% complete\n",
            "Progress: 32.84% complete\n",
            "Progress: 33.09% complete\n",
            "Progress: 33.33% complete\n",
            "Progress: 33.58% complete\n",
            "Progress: 33.83% complete\n",
            "Progress: 34.07% complete\n",
            "Progress: 34.32% complete\n",
            "Progress: 34.57% complete\n",
            "Progress: 34.81% complete\n",
            "Progress: 35.06% complete\n",
            "Progress: 35.31% complete\n",
            "Progress: 35.56% complete\n",
            "Progress: 35.80% complete\n",
            "Progress: 36.05% complete\n",
            "Progress: 36.30% complete\n",
            "Progress: 36.54% complete\n",
            "Progress: 36.79% complete\n",
            "Progress: 37.04% complete\n",
            "Progress: 37.28% complete\n",
            "Progress: 37.53% complete\n",
            "Progress: 37.78% complete\n",
            "Progress: 38.02% complete\n",
            "Progress: 38.27% complete\n",
            "Progress: 38.52% complete\n",
            "Progress: 38.77% complete\n",
            "Progress: 39.01% complete\n",
            "Progress: 39.26% complete\n",
            "Progress: 39.51% complete\n",
            "Progress: 39.75% complete\n",
            "Progress: 40.00% complete\n",
            "Progress: 40.25% complete\n",
            "Progress: 40.49% complete\n",
            "Progress: 40.74% complete\n",
            "Progress: 40.99% complete\n",
            "Progress: 41.23% complete\n",
            "Progress: 41.48% complete\n",
            "Progress: 41.73% complete\n",
            "Progress: 41.98% complete\n",
            "Progress: 42.22% complete\n",
            "Progress: 42.47% complete\n",
            "Progress: 42.72% complete\n",
            "Progress: 42.96% complete\n",
            "Progress: 43.21% complete\n",
            "Progress: 43.46% complete\n",
            "Progress: 43.70% complete\n",
            "Progress: 43.95% complete\n",
            "Progress: 44.20% complete\n",
            "Progress: 44.44% complete\n",
            "Progress: 44.69% complete\n",
            "Progress: 44.94% complete\n",
            "Progress: 45.19% complete\n",
            "Progress: 45.43% complete\n",
            "Progress: 45.68% complete\n",
            "Progress: 45.93% complete\n",
            "Progress: 46.17% complete\n",
            "Progress: 46.42% complete\n",
            "Progress: 46.67% complete\n",
            "Progress: 46.91% complete\n",
            "Progress: 47.16% complete\n",
            "Progress: 47.41% complete\n",
            "Progress: 47.65% complete\n",
            "Progress: 47.90% complete\n",
            "Progress: 48.15% complete\n",
            "Progress: 48.40% complete\n",
            "Progress: 48.64% complete\n",
            "Progress: 48.89% complete\n",
            "Progress: 49.14% complete\n",
            "Progress: 49.38% complete\n",
            "Progress: 49.63% complete\n",
            "Progress: 49.88% complete\n",
            "Progress: 50.12% complete\n",
            "Progress: 50.37% complete\n",
            "Progress: 50.62% complete\n",
            "Progress: 50.86% complete\n",
            "Progress: 51.11% complete\n",
            "Progress: 51.36% complete\n",
            "Progress: 51.60% complete\n",
            "Progress: 51.85% complete\n",
            "Progress: 52.10% complete\n",
            "Progress: 52.35% complete\n",
            "Progress: 52.59% complete\n",
            "Progress: 52.84% complete\n",
            "Progress: 53.09% complete\n",
            "Progress: 53.33% complete\n",
            "Progress: 53.58% complete\n",
            "Progress: 53.83% complete\n",
            "Progress: 54.07% complete\n",
            "Progress: 54.32% complete\n",
            "Progress: 54.57% complete\n",
            "Progress: 54.81% complete\n",
            "Progress: 55.06% complete\n",
            "Progress: 55.31% complete\n",
            "Progress: 55.56% complete\n",
            "Progress: 55.80% complete\n",
            "Progress: 56.05% complete\n",
            "Progress: 56.30% complete\n",
            "Progress: 56.54% complete\n",
            "Progress: 56.79% complete\n",
            "Progress: 57.04% complete\n",
            "Progress: 57.28% complete\n",
            "Progress: 57.53% complete\n",
            "Progress: 57.78% complete\n",
            "Progress: 58.02% complete\n",
            "Progress: 58.27% complete\n",
            "Progress: 58.52% complete\n",
            "Progress: 58.77% complete\n",
            "Progress: 59.01% complete\n",
            "Progress: 59.26% complete\n",
            "Progress: 59.51% complete\n",
            "Progress: 59.75% complete\n",
            "Progress: 60.00% complete\n",
            "Progress: 60.25% complete\n",
            "Progress: 60.49% complete\n",
            "Progress: 60.74% complete\n",
            "Progress: 60.99% complete\n",
            "Progress: 61.23% complete\n",
            "Progress: 61.48% complete\n",
            "Progress: 61.73% complete\n",
            "Progress: 61.98% complete\n",
            "Progress: 62.22% complete\n",
            "Progress: 62.47% complete\n",
            "Progress: 62.72% complete\n",
            "Progress: 62.96% complete\n",
            "Progress: 63.21% complete\n",
            "Progress: 63.46% complete\n",
            "Progress: 63.70% complete\n",
            "Progress: 63.95% complete\n",
            "Progress: 64.20% complete\n",
            "Progress: 64.44% complete\n",
            "Progress: 64.69% complete\n",
            "Progress: 64.94% complete\n",
            "Progress: 65.19% complete\n",
            "Progress: 65.43% complete\n",
            "Progress: 65.68% complete\n",
            "Progress: 65.93% complete\n",
            "Progress: 66.17% complete\n",
            "Progress: 66.42% complete\n",
            "Progress: 66.67% complete\n",
            "Progress: 66.91% complete\n",
            "Progress: 67.16% complete\n",
            "Progress: 67.41% complete\n",
            "Progress: 67.65% complete\n",
            "Progress: 67.90% complete\n",
            "Progress: 68.15% complete\n",
            "Progress: 68.40% complete\n",
            "Progress: 68.64% complete\n",
            "Progress: 68.89% complete\n",
            "Progress: 69.14% complete\n",
            "Progress: 69.38% complete\n",
            "Progress: 69.63% complete\n",
            "Progress: 69.88% complete\n",
            "Progress: 70.12% complete\n",
            "Progress: 70.37% complete\n",
            "Progress: 70.62% complete\n",
            "Progress: 70.86% complete\n",
            "Progress: 71.11% complete\n",
            "Progress: 71.36% complete\n",
            "Progress: 71.60% complete\n",
            "Progress: 71.85% complete\n",
            "Progress: 72.10% complete\n",
            "Progress: 72.35% complete\n",
            "Progress: 72.59% complete\n",
            "Progress: 72.84% complete\n",
            "Progress: 73.09% complete\n",
            "Progress: 73.33% complete\n",
            "Progress: 73.58% complete\n",
            "Progress: 73.83% complete\n",
            "Progress: 74.07% complete\n",
            "Progress: 74.32% complete\n",
            "Progress: 74.57% complete\n",
            "Progress: 74.81% complete\n",
            "Progress: 75.06% complete\n",
            "Progress: 75.31% complete\n",
            "Progress: 75.56% complete\n",
            "Progress: 75.80% complete\n",
            "Progress: 76.05% complete\n",
            "Progress: 76.30% complete\n",
            "Progress: 76.54% complete\n",
            "Progress: 76.79% complete\n",
            "Progress: 77.04% complete\n",
            "Progress: 77.28% complete\n",
            "Progress: 77.53% complete\n",
            "Progress: 77.78% complete\n",
            "Progress: 78.02% complete\n",
            "Progress: 78.27% complete\n",
            "Progress: 78.52% complete\n",
            "Progress: 78.77% complete\n",
            "Progress: 79.01% complete\n",
            "Progress: 79.26% complete\n",
            "Progress: 79.51% complete\n",
            "Progress: 79.75% complete\n",
            "Progress: 80.00% complete\n",
            "Progress: 80.25% complete\n",
            "Progress: 80.49% complete\n",
            "Progress: 80.74% complete\n",
            "Progress: 80.99% complete\n",
            "Progress: 81.23% complete\n",
            "Progress: 81.48% complete\n",
            "Progress: 81.73% complete\n",
            "Progress: 81.98% complete\n",
            "Progress: 82.22% complete\n",
            "Progress: 82.47% complete\n",
            "Progress: 82.72% complete\n",
            "Progress: 82.96% complete\n",
            "Progress: 83.21% complete\n",
            "Progress: 83.46% complete\n",
            "Progress: 83.70% complete\n",
            "Progress: 83.95% complete\n",
            "Progress: 84.20% complete\n",
            "Progress: 84.44% complete\n",
            "Progress: 84.69% complete\n",
            "Progress: 84.94% complete\n",
            "Progress: 85.19% complete\n",
            "Progress: 85.43% complete\n",
            "Progress: 85.68% complete\n",
            "Progress: 85.93% complete\n",
            "Progress: 86.17% complete\n",
            "Progress: 86.42% complete\n",
            "Progress: 86.67% complete\n",
            "Progress: 86.91% complete\n",
            "Progress: 87.16% complete\n",
            "Progress: 87.41% complete\n",
            "Progress: 87.65% complete\n",
            "Progress: 87.90% complete\n",
            "Progress: 88.15% complete\n",
            "Progress: 88.40% complete\n",
            "Progress: 88.64% complete\n",
            "Progress: 88.89% complete\n",
            "Progress: 89.14% complete\n",
            "Progress: 89.38% complete\n",
            "Progress: 89.63% complete\n",
            "Progress: 89.88% complete\n",
            "Progress: 90.12% complete\n",
            "Progress: 90.37% complete\n",
            "Progress: 90.62% complete\n",
            "Progress: 90.86% complete\n",
            "Progress: 91.11% complete\n",
            "Progress: 91.36% complete\n",
            "Progress: 91.60% complete\n",
            "Progress: 91.85% complete\n",
            "Progress: 92.10% complete\n",
            "Progress: 92.35% complete\n",
            "Progress: 92.59% complete\n",
            "Progress: 92.84% complete\n",
            "Progress: 93.09% complete\n",
            "Progress: 93.33% complete\n",
            "Progress: 93.58% complete\n",
            "Progress: 93.83% complete\n",
            "Progress: 94.07% complete\n",
            "Progress: 94.32% complete\n",
            "Progress: 94.57% complete\n",
            "Progress: 94.81% complete\n",
            "Progress: 95.06% complete\n",
            "Progress: 95.31% complete\n",
            "Progress: 95.56% complete\n",
            "Progress: 95.80% complete\n",
            "Progress: 96.05% complete\n",
            "Progress: 96.30% complete\n",
            "Progress: 96.54% complete\n",
            "Progress: 96.79% complete\n",
            "Progress: 97.04% complete\n",
            "Progress: 97.28% complete\n",
            "Progress: 97.53% complete\n",
            "Progress: 97.78% complete\n",
            "Progress: 98.02% complete\n",
            "Progress: 98.27% complete\n",
            "Progress: 98.52% complete\n",
            "Progress: 98.77% complete\n",
            "Progress: 99.01% complete\n",
            "Progress: 99.26% complete\n",
            "Progress: 99.51% complete\n",
            "Progress: 99.75% complete\n",
            "Progress: 100.00% complete\n",
            "\n",
            "Tokenization complete.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = Tokenizer2()\n",
        "# Transformation de chaque texte dans X_train en une séquence d'entiers\n",
        "sequences, token_number = tokenizer.tokenize(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1832, 1832, 1832, 1832, 1832, 1832, 1832, 1832, 1832, 1832, 1832, 1832, 1832, 1832, 1832, 1920, 1920, 1920, 1920, 1920, 1920, 1920, 1920, 1920, 1954, 1954, 1954, 1954, 1954, 1954, 1954, 1954, 1954, 1954, 1954, 1954, 1954, 1954, 1954, 1954, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1870, 1870, 1870, 1870, 1870, 1870, 1870, 1870, 1870, 1870, 1870, 1870, 1870, 1870, 1870, 1870, 1870, 1870, 1870, 1870, 1923, 1923, 1923, 1923, 1923, 1923, 1923, 1923, 1923, 1923, 1923, 1923, 1923, 1923, 1923, 1923, 1923, 1923, 1923, 1923, 1923, 1923, 1866, 1866, 1866, 1866, 1866, 1866, 1866, 1866, 1866, 1866, 1866, 1866, 1866, 1866, 1866, 1866, 1866, 1866, 1866, 1866, 1866, 1866, 1866, 1866, 1866, 1866, 1866, 1866, 1866, 1866, 1866, 1866, 1866, 1866, 1986, 1986, 1986, 1986, 1986, 1986, 1986, 1986, 1986, 1986, 1933, 1933, 1933, 1933, 1933, 1933, 1933, 1933, 1933, 1933, 1933, 1933, 1933, 1933, 1933, 1933, 1933, 1933, 1933, 1933, 1933, 1933, 1933, 1933, 1933, 1933, 1933, 1933, 1933, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 1897, 1897, 1897, 1897, 1897, 1897, 1897, 1897, 1897, 1897, 1897, 1897, 1897, 1897, 1897, 1897, 1897, 1897, 1897, 1897, 1897, 1897, 1987, 1987, 1987, 1987, 1987, 1987, 1987, 1987, 1987, 1987, 1987, 1987, 1987, 1987, 1987, 1987, 1987, 1987, 1987, 1987, 1987, 1987, 1987, 1882, 1882, 1882, 1882, 1882, 1882, 1882, 1882, 1882, 1882, 1882, 1882, 1882, 1882, 1882, 1882, 1882, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1910, 1910, 1910, 1910, 1910, 1910, 1910, 1910, 1910, 1910, 1910, 1908, 1908, 1908, 1908, 1908, 1908, 1908, 1908, 1908, 1908, 1908, 1908, 1908, 1908, 1908, 1908, 1895, 1830, 1830, 1830, 1830, 1830, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 1989, 1989, 1989, 1989, 1989, 1989, 1989, 1989, 1989, 1989, 1989, 1989, 1989, 1989, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1967, 1967, 1967, 1967, 1967, 1967, 1967, 1967, 1967, 1967, 1967, 1967, 1967, 1964, 1964, 1964, 1964, 1964, 1964, 1964, 1964, 1964, 1964, 1964, 1964, 1909, 1909, 1909, 1909, 1909, 1909, 1909, 1909, 1909, 1909, 1909, 1909, 1909, 1909, 1909, 1909, 1909, 1909, 1909, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1907, 1907, 1907, 1907, 1907, 1907, 1907, 1907, 1907, 1907, 1907, 1907, 1907, 1907, 1907, 1907, 1907, 1907, 1907, 1907, 1907, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 1920, 1920, 1920, 1920, 1920, 1920, 1920, 1920, 1920, 1920, 1920, 1920, 1920, 1949, 1949, 1949, 1949, 1949, 1949, 1949, 1949, 1958, 1958, 1988, 1988, 1988, 1988, 1988, 1988, 1988, 1988, 1988, 1988, 1988, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1954, 1954, 1954, 1954, 1954, 1954, 1954, 1954, 1912, 1912, 1912, 1912, 1912, 1912, 1912, 1912, 1912, 1912, 1912, 1912, 1990, 1990, 1990, 1990, 1990, 1990, 1990, 1990, 1990, 1990, 1990, 1990, 1948, 1948, 1948, 1948, 1934, 1934, 1934, 1934, 1934, 1934, 1934, 1934, 1934, 1934, 1934, 1934, 1934, 1934, 1934, 1934, 1934, 1934, 1934, 1934, 1934, 1934, 1934, 1934, 1934, 1934, 1934, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1908, 1908, 1908, 1908, 1908, 1908, 1908, 1908, 1908, 1908, 1908, 1908, 1908, 1908, 1908, 1908, 1908, 1908, 1908, 1908, 1908, 1908, 1908, 1908, 1908, 1908, 1949, 1949, 1949, 1949, 1949, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 1898, 1898, 1898, 1898, 1898, 1898, 1927, 1927, 1927, 1927, 1927, 1927, 1927, 1927, 1927, 1927, 1927, 1927, 1927, 1927, 1927, 1927, 1927, 1927, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 1843, 1843, 1843, 1843, 1843, 1843, 1843, 1843, 1843, 1843, 1843, 1970, 1970, 1970, 1970, 1992, 1992, 1992, 1992, 1925, 1925, 1925, 1925, 1925, 1925, 1925, 1695, 1695, 1695, 1695, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 1978, 1978, 1978, 1978, 1978, 1978, 1978, 1978, 1978, 1978, 1978, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1833, 1833, 1833, 1833, 1833, 1833, 1833, 1833, 1833, 1833, 1833, 1833, 1833, 1833, 1833, 1833, 1927, 1927, 1927, 1927, 1927, 1927, 1927, 1927, 1927, 1887, 1887, 1887, 1887, 1887, 1887, 1923, 1923, 1923, 1923, 1923, 1923, 1923, 1923, 1921, 1921, 1921, 1921, 1921, 1921, 1921, 1921, 1921, 1979, 1979, 1979, 1979, 1979, 1979, 1979, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 1964, 1964, 1964, 1964, 1964, 1964, 1964, 1964, 1964, 1964, 1964, 1964, 1964, 1964, 1964, 1964, 1964, 1964, 1964, 1964, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 1990, 1990, 1990, 1990, 1990, 1990, 1990, 1990, 1990, 1990, 1990, 1990, 1990, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 1852, 1852, 1852, 1852, 1852, 1852, 1852, 1852, 1852, 1852, 1852, 1852, 1852, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1900, 1971, 1971, 1971, 1971, 1971, 1971, 1971, 1971, 1971, 1971, 1971, 1971, 1971, 1971, 1971, 1971, 1971, 1971, 1971, 1971, 1971, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 1941, 1941, 1941, 1941, 1941, 1941, 1941, 1941, 1941, 1941, 1941, 1941, 1941, 1941, 1941, 1941, 1941, 1941, 1902, 1902, 1902, 1902, 1902, 1902, 1902, 1902, 1902, 1902, 1902, 1902, 1902, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1975, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 1893, 1893, 1893, 1893, 1943, 1943, 1943, 1943, 1943, 1943, 1943, 1943, 1943, 1943, 1943, 1943, 1943, 1943, 1943, 1943, 1943, 1905, 1905, 1905, 1905, 1905, 1905, 1905, 1905, 1905, 1905, 1905, 1905, 1905, 1905, 1905, 1905, 1905, 1888, 1888, 1888, 1888, 1888, 1888, 1888, 1888, 1888, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 1833, 1833, 1833, 1833, 1833, 1833, 1833, 1833, 1833, 1833, 1833, 1833, 1833, 1833, 1833, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 1901, 1901, 1901, 1901, 1901, 1901, 1901, 1901, 1901, 1901, 1901, 1901, 1901, 1901, 1901, 1901, 1901, 1901, 1903, 1903, 1903, 1903, 1903, 1903, 1903, 1903, 1903, 1903, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1957, 1957, 1957, 1957, 1957, 1957, 1957, 1957, 1957, 1957, 1957, 1957, 1957, 1957, 1957, 1957, 1957, 1957, 1957, 1957, 1957, 1957, 1895, 1895, 1895, 1895, 1895, 1895, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 1862, 1862, 1862, 1862, 1862, 1862, 1862, 1862, 1862, 1862, 1862, 1862, 1862, 1862, 1920, 1920, 1920, 1920, 1920, 1920, 1920, 1920, 1920, 1920, 1916, 1916, 1916, 1916, 1916, 1916, 1916, 1916, 1916, 1916, 1916, 1916, 1916, 1916, 1916, 1916, 1916, 1916, 1916, 1916, 1916, 1916, 1916, 1916, 1916, 1916, 1916, 1916, 1860, 1860, 1860, 1860, 1860, 1860, 1860, 1860, 1860, 1860, 1860, 1860, 1860, 1860, 1860, 1860, 1860, 1860, 1860, 1860, 1860, 1860, 1860, 1860, 1860, 1860, 1892, 1892, 1892, 1892, 1892, 1892, 1892, 1892, 1772, 1772, 1772, 1772, 1772, 1772, 1926, 1926, 1926, 1926, 1926, 1926, 1926, 1926, 1926, 1926, 1926, 1926, 1926, 1926, 1926, 1926, 1926, 1926, 1926, 1926, 1926, 1926, 1926, 1965, 1965, 1965, 1965, 1965, 1965, 1965, 1965, 1965, 1965, 1965, 1965, 1965, 1965, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 1964, 1964, 1964, 1964, 1964, 1964, 1964, 1964, 1964, 1964, 1964, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1917, 1917, 1917, 1917, 1917, 1917, 1917, 1917, 1917, 1917, 1917, 1917, 1917, 1917, 1917, 1917, 1917, 1917, 1917, 1917, 1917, 1917, 1917, 1917, 1917, 1917, 1917, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1788, 1788, 1788, 1788, 1788, 1788, 1788, 1788, 1788, 1788, 1788, 1788, 1788, 1788, 1788, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1857, 1857, 1857, 1857, 1857, 1857, 1857, 1857, 1857, 1857, 1857, 1857, 1857, 1857, 1857, 1857, 1947, 1947, 1947, 1947, 1947, 1947, 1947, 1947, 1947, 1947, 1947, 1947, 1947, 1947, 1947, 1947, 1947, 1947, 1947, 1924, 1924, 1924, 1924, 1924, 1924, 1924, 1924, 1924, 1924, 1924, 1924, 1924, 1924, 1929, 1929, 1929, 1929, 1929, 1929, 1929, 1929, 1929, 1929, 1929, 1929, 1983, 1983, 1983, 1983, 1983, 1983, 1983, 1983, 1983, 1983, 1983, 1983, 1983, 1983, 1983, 1983, 1983, 1983, 1983, 1983, 1983, 1983, 1983, 1983, 1983, 1983, 1983, 1983, 1983, 1983, 1922, 1922, 1922, 1922, 1922, 1922, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 1867, 1867, 1867, 1867, 1867, 1867, 1867, 1867, 1867, 1867, 1867, 1867, 1911, 1911, 1911, 1911, 1911, 1911, 1911, 1911, 1911, 1911, 1911, 1911, 1911, 1911, 1940, 1940, 1940, 1940, 1940, 1940, 1940, 1940, 1940, 1978, 1978, 1978, 1978, 1978, 1978, 1978, 1978, 1978, 1978, 1978, 1978, 1978, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1929, 1929, 1929, 1929, 1929, 1929, 1967, 1967, 1967, 1967, 1967, 1967, 1967, 1967, 1967, 1967, 1884, 1884, 1884, 1884, 1884, 1884, 1884, 1884, 1884, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 1897, 1897, 1897, 1897, 1897, 1897, 1897, 1897, 1897, 1897, 1897, 1897, 1897, 1897, 1897, 1897, 1897, 1897, 1948, 1948, 1948, 1948, 1948, 1948, 1948, 1948, 1948, 1948, 1948, 1948, 1948, 1948, 1948, 1948, 1948, 1991, 1991, 1991, 1991, 1991, 1991, 1991, 1991, 1991, 1991, 1991, 1991, 1991, 1991, 1991, 1991, 1991, 1991, 1991, 1991, 1991, 1991, 1991, 1991, 1991, 1991, 1991, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1854, 1854, 1854, 1854, 1854, 1949, 1949, 1949, 1949, 1949, 1949, 1949, 1949, 1949, 1949, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 1856, 1856, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 1902, 1902, 1902, 1902, 1902, 1902, 1902, 1902, 1902, 1902, 1902, 1902, 1902, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1902, 1902, 1902, 1902, 1902, 1902, 1902, 1902, 1902, 1902, 1902, 1902, 1902, 1902, 1902, 1902, 1902, 1902, 1902, 1902, 1902, 1902, 1902, 1902, 1902, 1902, 1902, 1902, 1902, 1902, 1888, 1888, 1888, 1888, 1888, 1888, 1888, 1888, 1888, 1888, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 1834, 1834, 1834, 1834, 1834, 1834, 1834, 1834, 1834, 1834, 1834, 1834, 1926, 1926, 1926, 1926, 1926, 1926, 1926, 1926, 1926, 1926, 1926, 1926, 1926, 1926, 1926, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 1865, 1865, 1865, 1865, 1865, 1865, 1865, 1865, 1865, 1865, 1865, 1865, 1865, 1865, 1865, 1865, 1865, 1865, 1865, 1865, 1865, 1865, 1865, 1865, 1865, 1865, 1865, 1865, 1865, 1865, 1865, 1865, 1865, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 1874, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 1945, 1945, 1945, 1945, 1945, 1945, 1945, 1945, 1945, 1945, 1945, 1945, 1945, 1945, 1945, 1945, 1945, 1945, 1945, 1945, 1945, 1945, 1945, 1945, 1945, 1945, 1945, 1945, 1945, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 1924, 1924, 1924, 1924, 1924, 1924, 1924, 1924, 1924, 1924, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1988, 1988, 1988, 1988, 1988, 1988, 1988, 1988, 1988, 1988, 1988, 1946, 1946, 1946, 1946, 1946, 1946, 1946, 1946, 1946, 1946, 1946, 1946, 1946, 1946, 1946, 1946, 1946, 1927, 1927, 1927, 1927, 1927, 1927, 1927, 1927, 1927, 1927, 1927, 1927, 1927, 1927, 1927, 1927, 1927, 1927, 1927, 1927, 1927, 1927, 1927, 1927, 1927, 1927, 1927, 1882, 1913, 1913, 1913, 1913, 1913, 1913, 1913, 1913, 1913, 1913, 1913, 1913, 1913, 1913, 1913, 1913, 2013, 2013, 2013, 2013, 2013, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 1953, 1953, 1953, 1953, 1953, 1953, 1953, 1953, 1953, 1953, 1953, 1953, 1953, 1953, 1953, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1963, 1952, 1952, 1952, 1952, 1952, 1952, 1952, 1952, 1952, 1952, 1888, 1888, 1888, 1888, 1888, 1888, 1888, 1888, 1888, 1888, 1888, 1888, 1888, 1888, 1636, 1636, 1636, 1636, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 1992, 1992, 1992, 1992, 1992, 1992, 1992, 1992, 1992, 1992, 1992, 1992, 1992, 1992, 1827, 1827, 1827, 1827, 1827, 1827, 1827, 1827, 1827, 1827, 1827, 1951, 1951, 1951, 1951, 1951, 1951, 1892, 1892, 1892, 1892, 1892, 1837, 1837, 1837, 1837, 1837, 1837, 1837, 1837, 1837, 1837, 1837, 1837, 1837, 1837, 1837, 1837, 1837, 1837, 1837, 1837, 1837, 1837, 1837, 1837, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 1911, 1911, 1911, 1911, 1911, 1911, 1911, 1911, 1911, 1911, 1911, 1911, 1911, 1911, 1911, 1911, 1911, 1911, 1911, 1911, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 1895, 1895, 1895, 1895, 1895, 1895, 1895, 1895, 1895, 1895, 1895, 1895, 1895, 1895, 1943, 1943, 1943, 1943, 1943, 1943, 1943, 1943, 1943, 1943, 1943, 1943, 1943, 1943, 1943, 1951, 1951, 1951, 1951, 1951, 1882, 1882, 1882, 1882, 1882, 1882, 1882, 1882, 1882, 1882, 1854, 1854, 1854, 1854, 1854, 1854, 1854, 1854, 1854, 1854, 1854, 1854, 1854, 1854, 1854, 1854, 1854, 1854, 1854, 1854, 1854, 1854, 1854, 1854, 1854, 1854, 1854, 1854, 1854, 1854, 1814, 1814, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1904, 1904, 1904, 1904, 1904, 1904, 1904, 1904, 1904, 1904, 1904, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 1967, 1967, 1967, 1967, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1910, 1910, 1910, 1910, 1910, 1910, 1910, 1910, 1941, 1941, 1941, 1941, 1941, 1941, 1941, 1941, 1941, 1919, 1919, 1919, 1846, 1846, 2021, 2021, 2021, 2021, 2021, 2021, 2021, 2021, 2021, 2021, 2021, 2021, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 1928, 1928, 1928, 1928, 1928, 1928, 1928, 1928, 1928, 1928, 1928, 1928, 1928, 1928, 1928, 1928, 1928, 1928, 1928, 1928, 1928, 1928, 1928, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1900, 1900, 1900, 1900, 1900, 1955, 1955, 1955, 1955, 1955, 1955, 1955, 1955, 1955, 1955, 1955, 1953, 1953, 1953, 1953, 1953, 1953, 1953, 1953, 1953, 1953, 1894, 1894, 1894, 1894, 1894, 1894, 1894, 1894, 1861, 1861, 1861, 1861, 1861, 1861, 1986, 1986, 1986, 1986, 1986, 1986, 1986, 1986, 1986, 1986, 1972, 1972, 1972, 1972, 1972, 1972, 1972, 1972, 1972, 1972, 1972, 1972, 1972, 1972, 1972, 1972, 1972, 1972, 1972, 1972, 1972, 1972, 1972, 1972, 1972, 1972, 1972, 1972, 1972, 1972, 1972, 1972, 1972, 1966, 1966, 1966, 1966, 1966, 1966, 1966, 1966, 1966, 1966, 1966, 1966, 1966, 1966, 1966, 1966, 1966, 1966, 1966, 1966, 1966, 1966, 1966, 1966, 1966, 1966, 1966, 1966, 1966, 1966, 1966, 1966, 1966, 1966, 1966, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1959, 1959, 1959, 1959, 1959, 1959, 1959, 1959, 1959, 1959, 1959, 1959, 1959, 1959, 1959, 1959, 1959, 1959, 1959, 1959, 1959, 1959, 1959, 1959, 1959, 1951, 1951, 1951, 1951, 1951, 1951, 1951, 1951, 1951, 1951, 1951, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 1929, 1929, 1929, 1929, 1929, 1929, 1929, 1929, 1929, 1929, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1869, 1869, 1869, 1869, 1869, 1869, 1869, 1869, 1869, 1869, 1869, 1869, 1869, 1869, 1869, 1869, 1869, 1869, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 1831, 1831, 1920, 1920, 1920, 1912, 1912, 1912, 1912, 1912, 1912, 1912, 1912, 1912, 1912, 1912, 1912, 1912, 1912, 1912, 1912, 1912, 1912, 1912, 1961, 1961, 1961, 1961, 1961, 1961, 1961, 1961, 1961, 1961, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1893, 1893, 1893, 1893, 1893, 1893, 1955, 1955, 1955, 1955, 1955, 1955, 1955, 1955, 1955, 1955, 1955, 1955, 1955, 1955, 1955, 1955, 1955, 1955, 1955, 1955, 1955, 1955, 1955, 1955, 1921, 1921, 1886, 1886, 1973, 1973, 1973, 1973, 1973, 1973, 1973, 1973, 1973, 1973, 1973, 1973, 1973, 1973, 1973, 1973, 1973, 1973, 1973, 1973, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 1863, 1863, 1863, 1863, 1863, 1863, 1863, 1863, 1863, 1863, 1863, 1863, 1863, 1863, 1863, 1863, 1863, 1863, 1863, 1863, 1863, 1863, 1863, 1863, 1863, 1863, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 1887, 1887, 1887, 1928, 1928, 1928, 1928, 1928, 1928, 1928, 1928, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1962, 1934, 1934, 1934, 1934, 1934, 1934, 1934, 1934, 1934, 1934, 1934, 1934, 1934, 1934, 1934, 1934, 1942, 1942, 1942, 1942, 1942, 1942, 1942, 1942, 1899, 1899, 1899, 1899, 1899, 1899, 1899, 1899, 1924, 1924, 1924, 1924, 1924, 1924, 1924, 1924, 1924, 1924, 1924, 1924, 1924, 1924, 1924, 1924, 1924, 1924, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891, 1891, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 1985, 2016, 2016, 2016, 2016, 1958, 1958, 1958, 1958, 1958, 1958, 1958, 1894, 1894, 1894, 1894, 1894, 1894, 1894, 1894, 1860, 1860, 1859, 1859, 1859, 1859, 1859, 1859, 1859, 1859, 1859, 1859, 1859, 1859, 1859, 1859, 1859, 1859, 1859, 1859, 1859, 1859, 1859, 1859, 1859, 1859, 1859, 1859, 1859, 1947, 1947, 1947, 1947, 1947, 1947, 1947, 1947, 1947, 1947, 1947, 1947, 1947, 1947, 1947, 1947, 1893, 1893, 1893, 1893, 1893, 1893, 1947, 1947, 1947, 1947, 1947, 1947, 1947, 1947, 1947, 1947, 1947, 1947, 1952, 1952, 1952, 1952, 1952, 1952, 1952, 1952, 1952, 1952, 1952, 1952, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 1965, 1965, 1965, 1965, 1965, 1965, 1965, 1965, 1965, 1965, 1965, 1965, 1965, 1965, 1965, 1965, 1965, 1965, 1965, 1965, 1965, 1965, 1965, 1965, 1899, 1899, 1899, 1899, 1899, 1899, 1899, 1986, 1986, 1986, 1986, 1986, 1986, 1986, 1986, 1986, 1986, 1935, 1935, 1935, 1935, 1935, 1935, 1935, 1935, 1935, 1935, 1935, 1935, 1935, 1935, 1935, 1935, 1935, 1894, 1894, 1894, 1894, 1894, 1894, 1894, 1894, 1894, 1894, 1894, 1894, 1839, 1839, 1839, 1839, 1839, 1839, 1839, 1839, 1839, 1839, 1839, 1839, 1839, 1839, 1839, 1839, 1839, 1839, 1839, 1839, 1839, 1839, 1839, 1839, 1839, 1839, 1839, 1839, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1885, 1885, 1885, 1885, 1885, 1885, 1885, 1885, 1885, 1885, 1885, 1885, 1885, 1885, 1885, 1885, 1885, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 1951, 1951, 1951, 1951, 1951, 1951, 1951, 1915, 1915, 1915, 1915, 1915, 1915, 1915, 1915, 1915, 1915, 1915, 1915, 1915, 1915, 1915, 1915, 1948, 1948, 1948, 1948, 1948, 1948, 1948, 1948, 1948, 1948, 1948, 1948, 1948, 1948, 1948, 1948, 1948, 1948, 1948, 1949, 1949, 1949, 1949, 1949, 1949, 1949, 1949, 1949, 1949, 1949, 1949, 1949, 1944, 1944, 1944, 1944, 1944, 1944, 1944, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1948, 1948, 1948, 1948, 1948, 1948, 1948, 1948, 1948, 1948, 1948, 1948, 1948, 1948, 1948, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1991, 1991, 1991, 1991, 1991, 1991, 1991, 1991, 1991, 1991, 1991, 1991, 1991, 1991, 1991, 1991, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 1869, 1869, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1923, 1923, 1923, 1923, 1923, 1923, 1923, 1923, 1923, 1923, 1923, 1923, 1923, 1923, 1923, 1923, 1884, 1884, 1884, 1884, 1884, 1884, 1884, 1884, 1884, 1884, 1884, 1884, 1884, 1884, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 1955, 1955, 1955, 1955, 1955, 1955, 1955, 1955, 1955, 1955, 1955, 1955, 1955, 1955, 1955, 1955, 1955, 1955, 1955, 1955, 1955, 1955, 1955, 1955, 1940, 1940, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 1932, 1932, 1932, 1932, 1932, 1932, 1932, 1932, 1932, 1932, 1932, 1932, 1932, 1932, 1932, 1932, 1932, 1932, 1932, 1932, 1932, 1932, 1932, 1932, 1932, 1932, 1932, 1932, 1932, 1932, 1932, 1932, 1932, 1932, 1932, 1932, 1932, 1932, 1932, 1932, 1932, 1932, 1932, 1932, 1932, 1946, 1946, 1946, 1946, 1946, 1946, 1946, 1946, 1946, 1946, 1946, 1946, 1946, 1946, 1946, 1946, 1946, 1860, 1860, 1860, 1860, 1860, 1860, 1860, 1860, 1860, 1860, 1860, 1860, 1860, 1637, 1637, 1637, 1637, 1958, 1958, 1958, 1958, 1958, 1958, 1958, 1958, 1958, 1958, 1958, 1958, 1958, 1958, 1958, 1958, 1958, 1958, 1958, 1958, 1958, 1942, 1942, 1942, 1942, 1942, 1942, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 1984, 1984, 1984, 1984, 1984, 1984, 1984, 1984, 1984, 1984, 1984, 1984, 1984, 1984, 1984, 1984, 1943, 1943, 1943, 1943, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1858, 1972, 1972, 1972, 1972, 1972, 1972, 1972, 1972, 1972, 1972, 1972, 1921, 1921, 1921, 1921, 1921, 1921, 1921, 1921, 1921, 1870, 1870, 1870, 1870, 1870, 1870, 1870, 1890, 1890, 1890, 1890, 1890, 1890, 1890, 1941, 1941, 1941, 1941, 1941, 1941, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 1881, 1881, 1881, 1881, 1881, 1881, 1881, 1881, 1881, 1881, 1881, 1881, 1881, 1881, 1881, 1881, 1881, 1881, 1881, 1933, 1933, 1933, 1933, 1933, 1933, 1933, 1933, 1933, 1978, 1978, 1978, 1978, 1978, 1978, 1978, 1978, 1978, 1978, 1862, 1862, 1862, 1862, 1862, 1862, 1862, 1862, 1862, 1862, 1862, 1862, 1862, 1862, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 1937, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 1905, 1905, 1905, 1905, 1905, 1905, 1905, 1905, 1905, 1905, 1905, 1903, 1903, 1903, 1903, 1903, 1903, 1903, 1903, 1903, 1903, 1903, 1859, 1859, 1859, 1859, 1859, 1859, 1859, 1859, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 1922, 1922, 1922, 1922, 1922, 1922, 1922, 1922, 1922, 1926, 1926, 1926, 1926, 1926, 1926, 1926, 1926, 1926, 1926, 1926, 1899, 1899, 1899, 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1975, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 1865, 1865, 1865, 1865, 1865, 1865, 1865, 1865, 1865, 1865, 1865, 1865, 1865, 1865, 1933, 1933, 1933, 1933, 1933, 1933, 1933, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 1883, 1933, 1933, 1933, 1933, 1933, 1933, 1933, 1933, 1933, 1933, 1933, 1933, 1933, 1933, 1933, 1933, 1933, 1933, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 1842, 1842, 1842, 1842, 1842, 1842, 1842, 1842, 1842, 1842, 1887, 1887, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1969, 1969, 1969, 1969, 1969, 1969, 1969, 1969, 1969, 1969, 1969, 1969, 1969, 1969, 1969, 1969, 1969, 1969, 1969, 1969, 1969, 1969, 1969, 1872, 1872, 1872, 1872, 1872, 1872, 1872, 1934, 1934, 1934, 1934, 1934, 1934, 1934, 1982, 1982, 1982, 1982, 1982, 1982, 1982, 1982, 1982, 1982, 1982, 1982, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 1843, 1843, 1843, 1843, 1843, 1843, 1843, 1843, 1843, 1843, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 1776, 1776, 1776, 1776, 1942, 1942, 1942, 1942, 1942, 1942, 1942, 1942, 1942, 1942, 1942, 1942, 1917, 1917, 1917, 1917, 1917, 1917, 1917, 1917, 1917, 1917, 1917, 1917, 2021, 2021, 2021, 2021, 2021, 2021, 2021, 2021, 2021, 2021, 2021, 2021, 2021, 2021, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1923, 1923, 1923, 1923, 1923, 1923, 1923, 1869, 1869, 1869, 1869, 1869, 1869, 1869, 1945, 1945, 1945, 1945, 1945, 1945, 1945, 1945, 1945, 1945, 1945, 1945, 1945, 1945, 1945, 1945, 1945, 1945, 1945, 1945, 1945, 1945, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1874, 1907, 1907, 1907, 1907, 1907, 1907, 1907, 1907, 1907, 1907, 1907, 1907, 1907, 1907, 1907, 1907, 1907, 1907, 1907, 1907, 1907, 1907, 1907, 1907, 1907, 1907, 1907, 1907, 1907, 1953, 1953, 1953, 1953, 1953, 1953, 1953, 1953, 1928, 1928, 1928, 1928, 1928, 1928, 1928, 1928, 1928, 1928, 1928, 1928, 1928, 1945, 1945, 1945, 1945, 1945, 1928, 1928, 1928, 1928, 1928, 1928, 1928, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1941, 1941, 1941, 1941, 1941, 1941, 1941, 1941, 1941, 1941, 1941, 1941, 1972, 1972, 1972, 1972, 1972, 1972, 1972, 1972, 1972, 1972, 1972, 1972, 1914, 1914, 1914, 1914, 1914, 1914, 1914, 1914, 1914, 1914, 1914, 1914, 1914, 1914, 1914, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1919, 1919, 1919, 1919, 1919, 1919, 1919, 1919, 1919, 1919, 1919, 1919, 1919, 1919, 1957, 1957, 1957, 1957, 1957, 1957, 1957, 1957, 1957, 1957, 1957, 1957, 1957, 1957, 1957, 1957, 1957, 1957, 1957, 1894, 1894, 1894, 1894, 1894, 1894, 1894, 1894, 1894, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1971, 1971, 1971, 1971, 1971, 1971, 1971, 1971, 1971, 1971, 1971, 1971, 1971, 1971, 1971, 1971, 1971, 1971, 1971, 1971, 1971, 1971, 1971, 1971, 1971, 1971, 1971, 1971, 1971, 1969, 1969, 1969, 1969, 1969, 1969, 1969, 1969, 1969, 1969, 1969, 1969, 1969, 1969, 1969, 1969, 1969, 1969, 1969, 1969, 1969, 1969, 1969, 1969, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1956, 1863, 1863, 1863, 1863, 1863, 1863, 1863, 1863, 1863, 1863, 1863, 1863, 1863, 1863, 1863, 1863, 1863, 1863, 1863, 1863, 1863, 1863, 1863, 1863, 1863, 1926, 1926, 1926, 1926, 1926, 1926, 1926, 1926, 1926, 1926, 1926, 1926, 1926, 1918, 1918, 1918, 1918, 1918, 1918, 1918, 1918, 1918, 1918, 1918, 1918, 1918, 1918, 1918, 1918, 1974, 1974, 1974, 1830, 1830, 1830, 1830, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1882, 1882, 1882, 1882, 1882, 1882, 1882, 1831, 1831, 1831, 1831, 1966, 1966, 1966, 1966, 1966, 1966, 1966, 1966, 1966, 1966, 1966, 1966, 1966, 1966, 1966, 1966, 1952, 1952, 1952, 1952, 1952, 1952, 1952, 1952, 1952, 1952, 1952, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 1847, 1847, 1847, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1936, 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 2007, 2007, 2007, 2007, 2007, 2007]\n",
            "6336\n",
            "Nombre d'index dans le tokenizer: 180957\n"
          ]
        }
      ],
      "source": [
        "data,y_train = diviser_liste(sequences,y_train,max_len)\n",
        "\n",
        "max_words = token_number\n",
        "print(y_train)\n",
        "# Convertir y_train en tableau numpy\n",
        "y_train = np.array(y_train)\n",
        "data = np.array(data)\n",
        "print(len(data))\n",
        "print(\"Nombre d'index dans le tokenizer:\", max_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHSKZeg-eM_T",
        "outputId": "80fffa33-cbf1-444b-9c66-b85bbb242ac5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "80783"
            ]
          },
          "execution_count": 126,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(sequences[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nquwvt_dB3hx"
      },
      "source": [
        "# Définition des architectures des modeles testés"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "45IEiu70Op4m"
      },
      "outputs": [],
      "source": [
        "# # Construction du modèle\n",
        "# # Pilee linéaire de couches. Dans ce modèle, les données passent à travers les couches dans l'ordre où elles ont été ajoutées.\n",
        "# model = Sequential()\n",
        "# # L'embedding est une représentation dense des mots (ou des tokens) dans un espace vectoriel.\n",
        "# # L'objectif principal de l'embedding est de capturer les similarités sémantiques entre les mots en les plaçant dans un espace vectoriel où les mots ayant des significations similaires sont plus proches les uns des autres.\n",
        "# model.add(Embedding(max_words, embedding_dim, input_length=max_len))\n",
        "\n",
        "# # Couche de convolution à une dimension (1D) au modèle. Cette couche va utiliser 128 filtres de taille 10 avec une fonction d'activation ReLU (Rectified Linear Unit).\n",
        "# # La convolution 1D est utilisée pour extraire des caractéristiques séquentielles des données.\n",
        "# model.add(Conv1D(128,10, activation='relu'))\n",
        "\n",
        "# # Le pooling global maximale extrait les caractéristiques les plus significatives de chaque canal de la sortie de la couche précédente, réduisant ainsi la dimensionnalité des données.\n",
        "# model.add(GlobalMaxPooling1D())\n",
        "\n",
        "# # Couche dense (entièrement connectée) au modèle avec 128 neurones et une fonction d'activation ReLU.\n",
        "# model.add(Dense(128, activation='relu'))\n",
        "\n",
        "# # Le dropout est une technique de régularisation qui désactive aléatoirement un pourcentage (dans ce cas, 50%) des neurones de la couche précédente pendant l'entraînement,\n",
        "# # ce qui aide à prévenir le surapprentissage.\n",
        "# model.add(Dropout(0.5))\n",
        "\n",
        "# # Couche dense de sortie au modèle avec un seul neurone et une fonction d'activation linéaire.\n",
        "# # Dans ce cas, la sortie est une seule valeur, qui est la date de publication prédite.\n",
        "# model.add(Dense(1, activation='linear'))\n",
        "\n",
        "# # La fonction ReLU est couramment utilisée dans les réseaux de neurones en raison de sa simplicité et de sa capacité à introduire une non-linéarité dans le modèle,\n",
        "# # ce qui permet au réseau de capturer des motifs complexes dans les données.\n",
        "\n",
        "# model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
        "# model.summary()\n",
        "\n",
        "# # Entraînement du modèle\n",
        "# model.fit(data, y_train, epochs=50, batch_size=32)\n",
        "# #, validation_split=0.2\n",
        "# # Après l'entraînement, vous pouvez utiliser le modèle pour prédire la date de publication d'un nouveau livre.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "RGosKjmyzmuW"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 50\n",
        "\n",
        "def archi_3CONV32_F_DE_DR_DE():\n",
        "  model = Sequential()\n",
        "\n",
        "#inputdim = le nombre de voc, inputlength le nombre delement dans une liste\n",
        "  model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len))\n",
        "  model.add(Conv1D(32, 9, activation='relu'))\n",
        "  model.add(Conv1D(32, 9, activation='relu'))\n",
        "  model.add(Conv1D(32, 9, activation='relu'))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(1e-6)))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(1))\n",
        "\n",
        "  return model\n",
        "\n",
        "def archi_3CONV16_F_DE_DR_DE():\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len))\n",
        "  model.add(Conv1D(16, 5, activation='relu'))\n",
        "  model.add(Conv1D(16, 5, activation='relu'))\n",
        "  model.add(Conv1D(16, 5, activation='relu'))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(1e-6)))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(1))\n",
        "  return model\n",
        "\n",
        "def archi_3CONV64_F_DE_DR_DE():\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len))\n",
        "  model.add(Conv1D(64, 9, activation='relu'))\n",
        "  model.add(Conv1D(64, 9, activation='relu'))\n",
        "  model.add(Conv1D(64, 9, activation='relu'))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(1e-6)))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(1))\n",
        "\n",
        "  return model\n",
        "\n",
        "def archi_3CONV64_32_16_F_DE_DR_DE():\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len))\n",
        "  model.add(Conv1D(64, 5, activation='relu'))\n",
        "  model.add(Conv1D(32, 5, activation='relu'))\n",
        "  model.add(Conv1D(16, 5, activation='relu'))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(1e-6)))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(1))\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYpHlL5b1x7u"
      },
      "source": [
        "# Prétraitement données de test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 2.00% complete\n",
            "Progress: 4.00% complete\n",
            "Progress: 6.00% complete\n",
            "Progress: 8.00% complete\n",
            "Progress: 10.00% complete\n",
            "Progress: 12.00% complete\n",
            "Progress: 14.00% complete\n",
            "Progress: 16.00% complete\n",
            "Progress: 18.00% complete\n",
            "Progress: 20.00% complete\n",
            "Progress: 22.00% complete\n",
            "Progress: 24.00% complete\n",
            "Progress: 26.00% complete\n",
            "Progress: 28.00% complete\n",
            "Progress: 30.00% complete\n",
            "Progress: 32.00% complete\n",
            "Progress: 34.00% complete\n",
            "Progress: 36.00% complete\n",
            "Progress: 38.00% complete\n",
            "Progress: 40.00% complete\n",
            "Progress: 42.00% complete\n",
            "Progress: 44.00% complete\n",
            "Progress: 46.00% complete\n",
            "Progress: 48.00% complete\n",
            "Progress: 50.00% complete\n",
            "Progress: 52.00% complete\n",
            "Progress: 54.00% complete\n",
            "Progress: 56.00% complete\n",
            "Progress: 58.00% complete\n",
            "Progress: 60.00% complete\n",
            "Progress: 62.00% complete\n",
            "Progress: 64.00% complete\n",
            "Progress: 66.00% complete\n",
            "Progress: 68.00% complete\n",
            "Progress: 70.00% complete\n",
            "Progress: 72.00% complete\n",
            "Progress: 74.00% complete\n",
            "Progress: 76.00% complete\n",
            "Progress: 78.00% complete\n",
            "Progress: 80.00% complete\n",
            "Progress: 82.00% complete\n",
            "Progress: 84.00% complete\n",
            "Progress: 86.00% complete\n",
            "Progress: 88.00% complete\n",
            "Progress: 90.00% complete\n",
            "Progress: 92.00% complete\n",
            "Progress: 94.00% complete\n",
            "Progress: 96.00% complete\n",
            "Progress: 98.00% complete\n",
            "Progress: 100.00% complete\n",
            "\n",
            "Tokenization complete.\n",
            "Progress: 2.22% complete\n",
            "Progress: 4.44% complete\n",
            "Progress: 6.67% complete\n",
            "Progress: 8.89% complete\n",
            "Progress: 11.11% complete\n",
            "Progress: 13.33% complete\n",
            "Progress: 15.56% complete\n",
            "Progress: 17.78% complete\n",
            "Progress: 20.00% complete\n",
            "Progress: 22.22% complete\n",
            "Progress: 24.44% complete\n",
            "Progress: 26.67% complete\n",
            "Progress: 28.89% complete\n",
            "Progress: 31.11% complete\n",
            "Progress: 33.33% complete\n",
            "Progress: 35.56% complete\n",
            "Progress: 37.78% complete\n",
            "Progress: 40.00% complete\n",
            "Progress: 42.22% complete\n",
            "Progress: 44.44% complete\n",
            "Progress: 46.67% complete\n",
            "Progress: 48.89% complete\n",
            "Progress: 51.11% complete\n",
            "Progress: 53.33% complete\n",
            "Progress: 55.56% complete\n",
            "Progress: 57.78% complete\n",
            "Progress: 60.00% complete\n",
            "Progress: 62.22% complete\n",
            "Progress: 64.44% complete\n",
            "Progress: 66.67% complete\n",
            "Progress: 68.89% complete\n",
            "Progress: 71.11% complete\n",
            "Progress: 73.33% complete\n",
            "Progress: 75.56% complete\n",
            "Progress: 77.78% complete\n",
            "Progress: 80.00% complete\n",
            "Progress: 82.22% complete\n",
            "Progress: 84.44% complete\n",
            "Progress: 86.67% complete\n",
            "Progress: 88.89% complete\n",
            "Progress: 91.11% complete\n",
            "Progress: 93.33% complete\n",
            "Progress: 95.56% complete\n",
            "Progress: 97.78% complete\n",
            "Progress: 100.00% complete\n",
            "\n",
            "Tokenization complete.\n"
          ]
        }
      ],
      "source": [
        "# Transformation de chaque texte dans X_train en une séquence d'entiers\n",
        "sequences_test, _ = tokenizer.tokenizeNoAdd(X_test)\n",
        "\n",
        "data_test,y_test = diviser_liste(sequences_test,y_test,max_len)\n",
        "\n",
        "# Convertir y_train en tableau numpy\n",
        "y_test = np.array(y_test)\n",
        "data_test = np.array(data_test)\n",
        "\n",
        "# Transformation de chaque texte dans X_train en une séquence d'entiers\n",
        "sequences_val, _ = tokenizer.tokenizeNoAdd(X_val)\n",
        "\n",
        "data_val,y_val = diviser_liste(sequences_val,y_val,max_len)\n",
        "\n",
        "# Convertir y_train en tableau numpy\n",
        "y_val = np.array(y_val)\n",
        "data_val = np.array(data_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tofi22Hb2KQQ"
      },
      "source": [
        "# Entrainement des models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5unk1gmM2O0y"
      },
      "source": [
        "## Trouver le meilleur"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vFZ2GKp2-Gbf",
        "outputId": "7af30246-57be-45a7-cb26-0794908193a6"
      },
      "outputs": [],
      "source": [
        "# # Définir la liste des modèles à évaluer\n",
        "# models = [archi_3CONV32_F_DE_DR_DE(), archi_3CONV16_F_DE_DR_DE(), archi_3CONV64_F_DE_DR_DE(), archi_3CONV64_32_16_F_DE_DR_DE()]\n",
        "\n",
        "# best_model = None\n",
        "# best_val_loss = float('inf')\n",
        "# best_val_mae = float('inf')\n",
        "\n",
        "# # Entraîner et évaluer chaque modèle\n",
        "# for model in models:\n",
        "#     model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
        "#     model.fit(data, y_train, epochs=20, batch_size=64, validation_data=(data_val, y_val))\n",
        "\n",
        "#     # Évaluer sur l'ensemble de test\n",
        "#     val_loss, val_mae = model.evaluate(data_test, y_test)\n",
        "\n",
        "#     # Mettre à jour le meilleur modèle si nécessaire\n",
        "#     if val_mae < best_val_mae:\n",
        "#         best_val_loss = val_loss\n",
        "#         best_val_mae = val_mae\n",
        "#         best_model = model\n",
        "\n",
        "# # Afficher les performances du meilleur modèle\n",
        "# print(\"Le meilleur modèle est:\")\n",
        "# best_model.summary()\n",
        "# print(\"Meilleure valeur de perte (loss) sur l'ensemble de test:\", best_val_loss)\n",
        "# print(\"Meilleure MAE (Mean Absolute Error) sur l'ensemble de test:\", best_val_mae)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "PdHvHupcADlh"
      },
      "outputs": [],
      "source": [
        "# model = best_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIvj5MS82Q2D"
      },
      "source": [
        "## Ou en entrainer un seul"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwx9k9Nc3Alv",
        "outputId": "4429e182-83ce-41d3-b151-a65d57bf6183"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_26\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_26 (Embedding)    (None, 5000, 50)          9047850   \n",
            "                                                                 \n",
            " conv1d_78 (Conv1D)          (None, 4992, 64)          28864     \n",
            "                                                                 \n",
            " conv1d_79 (Conv1D)          (None, 4984, 64)          36928     \n",
            "                                                                 \n",
            " conv1d_80 (Conv1D)          (None, 4976, 64)          36928     \n",
            "                                                                 \n",
            " flatten_26 (Flatten)        (None, 318464)            0         \n",
            "                                                                 \n",
            " dense_52 (Dense)            (None, 256)               81527040  \n",
            "                                                                 \n",
            " dropout_26 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_53 (Dense)            (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 90677867 (345.91 MB)\n",
            "Trainable params: 90677867 (345.91 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "99/99 [==============================] - 101s 1s/step - loss: 288177.3438 - mae: 340.5597 - val_loss: 3040.6792 - val_mae: 49.0845\n",
            "Epoch 2/20\n",
            "99/99 [==============================] - 105s 1s/step - loss: 38918.7344 - mae: 157.2867 - val_loss: 4944.1338 - val_mae: 59.6657\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'loss': [288177.34375, 38918.734375],\n",
              " 'mae': [340.5596923828125, 157.28672790527344],\n",
              " 'val_loss': [3040.67919921875, 4944.1337890625],\n",
              " 'val_mae': [49.08445739746094, 59.665740966796875]}"
            ]
          },
          "execution_count": 133,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = archi_3CONV64_F_DE_DR_DE()\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
        "model.summary()\n",
        "\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Création du callback EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='loss', patience=1)  # Vous pouvez ajuster la patience selon vos besoins\n",
        "\n",
        "# Entraînement du modèle avec EarlyStopping\n",
        "history = model.fit(data, y_train, epochs=20, batch_size=64, validation_data=(data_val, y_val), callbacks=[early_stopping])\n",
        "\n",
        "# L'historique de l'entraînement sera disponible dans history.history comme précédemment\n",
        "\n",
        "history.history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "23/23 [==============================] - 3s 108ms/step - loss: 4258.8364 - mae: 59.6753\n",
            "Validation Loss: 4258.83642578125\n",
            "Validation MAE: 59.67530059814453\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the test data\n",
        "val_loss, val_mae = model.evaluate(data_test, y_test)\n",
        "\n",
        "# Print the validation loss and validation mean absolute error\n",
        "print(\"Validation Loss:\", val_loss)\n",
        "print(\"Validation MAE:\", val_mae)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "5CK1OPbMz2vm",
        "bIvj5MS82Q2D",
        "s71plxAN2fbA"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
